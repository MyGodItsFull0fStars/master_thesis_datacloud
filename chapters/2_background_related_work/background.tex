\chapter{Background and Related Work}
\label{ch:background}

    \section{Monitoring}
    \label{sec:monitoring-background}

        Monitoring is the process of continuously observing and tracking a system, process, or activity in order to gather data, identify trends, and detect any deviations or issues.
        Monitoring has the purpose to ensure that the system is functioning as intended and also to identify any potential problems early on in order to mitigate them. Additionally, monitoring provides necessary information for effective decision making and problem solving.
        There are different monitoring types, including \emph{system-, environmental-, process-, performance- and security monitoring}.
        We mainly focus on \nameref{par:system-monitoring-background} and \nameref{par:process-monitoring-background}.
        Monitoring can be performed manually or through the use of automated tools and technologies.
        The choice of monitoring approach will depend on the specific requirements of the system or process being monitored.
        

        \paragraph{System Monitoring}
        \label{par:system-monitoring-background}

            System monitoring involves monitoring the performance and availability of computer systems, applications and networks.


        \paragraph{Process Monitoring}
        \label{par:process-monitoring-background}

            Process monitoring involves monitoring industrial processes to ensure they are operating efficiently and safely.




    \section{Computing Continuum}
    \label{sec:computing-continuum-background}

        Placeholder


    \section{Scheduling and Adaptation}
    \label{sec:scheduling-and-adaptation-background}

    \section{Prediction of Load}
    \label{sec:prediction-of-load-background}

        User based prediction via domain expertise

    \section{Supervised Learning}
    \label{sec:supervised-learning}

        Supervised learning is a machine learning paradigm that is applied when available data consists of labelled data points, that not only contain features but also a label that is associated with these features.
        Supervised learning is defined by its use of labelled datasets to train algorithms to classify data or predict outcomes accurately.
        As input data is fed into the model, it adjusts its weights until the model has been fitted appropriately, which occurs as part of the cross validation process.
        Supervised Learning uses a training set to teach models to yield the desired output.  
        This training dataset includes inputs and correct outputs, which allow the model to learn over time. The algorithm measures its accuracy through a loss function, adjusting until the error rate has been sufficiently minimized.
        A training dataset with $n$ are of the form $\left\{(x_1, y_1), \dots , (x_n, y_n)\right\}$ such that $x_i$ is defined as the \emph{feature vector} of the $i$-th data point and $y_i$ is defined as its label.
        A supervised learning algorithm will train to learn a function $g$ such that $g: X \rightarrow Y$, where $X$ is the input space and $Y$ is the output space.
        The function $g$ is often represented as a scoring function $f: X \times Y \rightarrow \mathcal{R}$ such that $g$ is defined as $g(x) = \arg \max f(x, y)$ such that $g$ returns the $y$ value with the highest score.
        Supervised learning can be separated into \nameref{sec:classification-supervised-learning-background} and \nameref{sec:regression-supervised-learning-background} problems.

        In supervised learning for scheduling the objective is to learn parameters of the neural network such that the trained networks can replicate the behaviour of pre-existing scheduling methods, such as mathematical optimisations, search methods, for example \emph{Priority Dispatching Rule (PDR)}, a heuristic rule that assigns jobs to machines based on their priorities while considering the current status fo the system.
        Supervised learning can also be used for forecast models that predict the resource utilisation or the runtime of tasks.
        This is done by providing information such as the capacity of a resource, the naive allocation prediction provided by a human and other factors to the feature vector and including the target that is to be predicted to the labels. 
        The training dataset for this prediction task is usually a time series forecasting dataset.
        In a time series forecasting dataset, the order of data points is ascending, meaning that the structure and order of the data points is used as an additional information that is used for the prediction.

        \subsection{Classification}
        \label{sec:classification-supervised-learning-background}

            A classification problem is when the output value is a categorical data type, such as "cat" and "dog".
            This section is added for completeness but won't be discussed further since it serves no purpose for this masters thesis.

        \subsection{Regression}
        \label{sec:regression-supervised-learning-background}

            Regression is used to predict continuous-valued outputs. That is, find a relationship between the input and output data generated by an unknown but fixed distribution represented by a function denoted as $f(x)$. 
            A regression model does so by showing whether the observed changes in the dependent variable are also associated with changes in the independent (explanatory) variables. 
            \begin{figure}[h!]
                \centering
                \includegraphics[scale=0.6]{figures/regression_plot.png}
                \caption{Regression Example}
                \label{fig:regression-example}
            \end{figure}
            In the figure \ref{fig:regression-example} an example regression is shown by the orange line that is done by a linear regression. As can be seen, the linear regression line is estimating the values of the data points by trying to fit closest to all data points that are shown as the blue dots.
            This is done by using the function $f(x)$ to find a best-fit line and see how much the data is dispersed around this line.
            \begin{quote}
                Regression is a statistical method used in finance, investing, and other disciplines that attempts to determine the strength and character of the relationship between one dependent variable (usually denoted by Y) and a series of other variables (known as independent variables). \cite{beersWhatRegressionDefinition}
            \end{quote}
            Therefore, regression analysis is a useful tool for finding associations of variables that are observed in data. Yet, those associations do not indicate causation but merely correlation.
            This is because regression only captures correlations between variables that are observed in data and quantifies if the found correlation are statistically significant.
            In order for proper interpretations regarding the regression results, several assumptions about the data and the model itself must hold.

    \section{Recurrent Neural Network}
    \label{sec:rnn-background}

        Recurrent Neural Networks (RNNs) are a type of neural network designed to handle sequential data.
        Opposed to traditional feedforward neural networks, which receive a fixed-sized vector as input and produce a fixed-sized vector as output, 
        RNNs are able to handle sequences of variable length and produce a hidden state that summarizes information from the entire sequence.
        \begin{figure}[h!]
            \centering
            \includegraphics[scale=0.5]{figures/FNN_vs_RNN.drawio.png}
            \caption{FNN vs. RNN}
            \label{fig:fnn-vs-rnn}
        \end{figure}
        In an RNN, each unit in the network processes the input sequence one time step at a time, maintaining a hidden state that summarizes information from all previous time steps. At each time step, the hidden state is updated based on both the current input and the hidden state from the previous time step. The hidden state is then used to generate the output for that time step, and the process is repeated for each time step in the sequence.
        RNNs are used for a variety of tasks, including natural language processing, speech recognition, and video analysis. They can also be used to generate sequences, such as in text generation or music composition.


        The ability to maintain information across a sequence of inputs makes RNNs a fitting tool for modelling sequential data.
        \begin{quote}
            ... recurrent neural networks contain cycles that feed the network activations from a previous time step as inputs to the network to influence predictions at the current time step. These activations are stored in the internal states of the network which can in principle hold long-term temporal contextual information. This mechanism allows RNNs to exploit a dynamically changing contextual window over the input sequence history. \cite{sakLongShortTermMemory2014}
        \end{quote}
        Yet, this also results in them being prone to \nameref{par:vanishing-gradients-background} and \nameref{par:exploding-gradients-background}, that make it cumbersome to train the network effectively. 
        To address these issues, several variants of RNNs have been developed, \nameref{sec:lstm-background} networks and Gated Recurrent Units (GRUs).
        
        
        \subsection{Recurrent Cell Architecture}
        \label{sec:recurrent-cell-architecture-background}

            A traditional feed-forward neural network (FNN) is unidirectional, meaning that they have a single direction and hence cannot persist information over a time step $t$.
            Looping structures are added to a feed-forward neural network that enable the persistence of information about time-series or sequential data. 
            This is the reason RNN's are known as "recurrent" neural networks.

            As can be seen in figure \ref{fig:fnn-vs-rnn}, the RNN has an additional loop inside it to persist time-series information. The loop structure enables the RNN to apply a \emph{recurrence relation} (see \ref{sec:recurrence-relation-background}) at every time step in order to process a sequence.

            The rectangle containing the "RNN" label is defined as a "recurrent cell".
            This workflow of a single RNN cell can be seen in more detail in figure \ref{fig:single-rnn-cell}, where the previous cell state $h_{t-1}$ and the current input $x_t$ are used as the input of the recurrent cell, get combined and forwarded to the $tanh$ \emph{activation function}, which returns the output vector $\hat{y}_t$ and $h_t$ (the recurrence relation).

        \subsection{Recurrence Relation}
        \label{sec:recurrence-relation-background}

            The recurrence relation is applied at every time step to process a sequence.

            The recurrent relation seen in figures \ref{fig:fnn-vs-rnn}, \ref{fig:single-rnn-cell} is denoted as $h_t$, and is defined as $h_t = f_w(h_{t - 1}, x_t)$, where $f_w$ is a function that is parametrized by the weights, $h_{t-1}$ is the previous state and $x_t$ is the input vector at time step $t$. With the addition of $h_{t-1}$, the model is now also taking the previous time step into account when updating the current time step. 
            \begin{figure}[h!]
                \centering
                \includegraphics[scale=0.5]{figures/single_rnn_cell.drawio.png}
                \caption{Single RNN Cell}
                \label{fig:single-rnn-cell}
            \end{figure}

            
            \begin{pabox}{Recurrence Relation}
                \label{def:lstm-recurrence-relation-definition}
                The recurrence relation in a more mathematical notation is:
                $$h_t = \tanh(W^T_{hh} h_{t-1} + W^T_{xh} x_t)$$
                Both the input vector $x_t$ as well as the previous state $h_{t-1}$ are multiplied with the two separate weight matrices $W^T_{xh}$ and $W^T{hh}$ respectively, combined and fed to the $\tanh$ activation function. Finally, the $\tanh$ function returns the output vector $\hat{y}_t$ at time step $t$.
            \end{pabox}
            

        \subsection{RNN Loop Unfolding}
        \label{sec:rnn-loop-unfolding-background}

            The loop unfolding will provide a description about how RNN handle sequential data at every time step.
            As can  be seen in figure \ref{fig:rnn-loop-unfolding}, the model is adding the input at every time step, and generating an output $\hat{y}_i$ for every time step.
            The weight matrices $W_{hh}$ that are used for every time step for updating the previous state are the same for every time step.
            The weight matrix $W_{xh}$ is applied to every $x_i$ and is also the same for every time step $i$.

            \begin{figure}[h!]
                \centering
                \includegraphics[width=0.90\textwidth]{figures/rnn_loop_unfolding.drawio.png}
                \caption{RNN Loop Unfolding}
                \label{fig:rnn-loop-unfolding}
            \end{figure}
            \begin{itemize}[label=\textemdash]
                \item $x_i$ denotes the input value at time step $i$.
                \item $\hat{y}_i$ denotes the output value at time step $i$.
                \item $W_{hh}$ denotes the weight matrix to update the previous state.
                \item $W_{xh}$ denotes the weight matrix that is applied to the input value at every time step.
            \end{itemize}
            The output vectors $\hat{y}_0, \hat{y}_1, \hat{y}_2, \dots, \hat{y}_t$ can be used to calculate the separate losses $L_0, L_1, L_2, \dots, L_t$ at each time step $t$.
            % This completes the forward propagation (see \ref{sec:forward-propagation}).

        \subsection{Loss Calculation and Weight Updates for RNN}
        \label{sec:loss-calculation-and-weight-updates-for-rnn-background}
        
            The training of an unfolded RNN is done through multiple time steps, as can be seen in figure \ref{fig:rnn-loop-unfolding}.
            The overall loss is defined as $L = L_0, L_1, \dots, L_t$ and is calculated from the outputs $\hat{y}_0, \hat{y}_1, \dots, \hat{y}_t$  for each time step $t$ in the \emph{forward-propagation} process.
            \begin{figure}[h!]
                \centering
                \includegraphics[width=0.6\textwidth]{figures/rnn_loss_calculation.drawio.png}
                \caption{Loss Calculation and Weight Update in RNNs}
                \label{fig:loss-calculation-weight-update-rnn}
            \end{figure}
            Then the total loss $L$ is used to propagate backwards and calculate the \emph{back-propagation} in order to update the weights of the model.
            This is shown in figure \ref{fig:loss-calculation-weight-update-rnn}, where the black arrows display the forward-propagation step that are accumulated as the total loss $L$, and then the back-propagation shown as the red arrows update the weights by using the partial derivative.

            \begin{quote}
                The central problem that back-propagation solves is the evaluation of the influence of a parameter on a function whose computation involves several elementary \emph{steps}. The solution to this problem is given by the chain rule, but back-propagation exploits the particular form of the functions used at each step (or layer) to provide an elegant and local procedure. \cite{lecunTheoreticalFrameworkBackpropagation1988}
            \end{quote}
            Back-propagation is the practice of fine-tuning the weights of a neural network based on the error rate (i.e. loss) obtained in the previous epoch. Proper tuning of the weights ensures lower error rates, making the model reliable by increasing its generalization.


        \subsection{Common Problems and Shortcomings of RNNs}
        \label{sec:shortcomings-of-rnns-background}

            Regularly experienced problems of RNNs are the exploding or vanishing gradient problems.
            \begin{quote}
                The motivation behind why they happen is that it is hard to catch long haul conditions as a result of a multiplicative angle that can be dramatically diminishing/expanding regarding the number of layers. \cite{parikhDisadvantagesRNN2021}
            \end{quote}
            While unfolding, the error gradient is calculated as the sum of all gradient errors across time steps. Therefore, the loss calculation in unfolded RNNs is also known as \emph{backpropagation through time (BPTT)}.
            Over time while calculating the error gradients the domination of the multiplicative term increases due to the chain rule application. And thus the gradients either explode or vanish.
            These problems occur when the sequence is too long and this may result in the model training with either null weights (i.e. the model won't learn while training) or exploding weights.
            
            \paragraph{Exploding Gradients}
            \label{par:exploding-gradients-background}

                The Exploding Gradients problem occurs when many of the values (i.e. weight matrices, or gradients themselves) involved in the repeated gradient computations are greater than 1. 
                If this is the case, then gradients become extremely large and optimising them becomes computationally intensive.
                To solve the Exploding Gradients problem, a process called \emph{Gradient Clipping} is applied, that scales the gradient values to smaller values less than 1.


            \paragraph{Vanishing Gradients}
            \label{par:vanishing-gradients-background}

                The Vanishing Gradients problem occurs when many of the values (i.e. weight matrices or gradients themselves) that are involved in the repeated gradient computations are too small or less than 1. Opposed to the \nameref{par:exploding-gradients-background} problem, the gradients become smaller with each repeated computation of the gradients.
                This results in the problem of \emph{long term dependency}, were smaller sequences can be remembered and the weights updated accordingly. But for longer sequences, the model will unlikely be able to yield a good prediction performance.

                There are multiple solutions to the Vanishing Gradient problem, such as changing the activation function from $\tanh$ to \emph{Rectified Linear Unit (ReLU)}. Also initialising the weights of the model can solve this problem, though tailored heuristics (such as the \emph{Xavier Weight Initialisation}) should be used to result in omitting the Vanishing Gradients problem and also increase the efficiency of the training. Another solution is to change the architecture of the neural network and adding more complex recurrent units that are mentioned in section \ref{sec:lstm-background}.

            \paragraph{Slow and Complex Training}

                Since RNNs are recurrent one of their fundamental problems is that they require a lot of time for training when compared to FNNs. Additionally, RNNs need to calibrate the previous outputs as well as current inputs into a state change function, which in turn make RNNs harder to implement and customize and more complex to train.

            \paragraph{Difficult to Process Longer Sequences}

                As already mentioned earlier, training RNNs on too long sequences is difficult without taking any measures to improve the prediction performance. This is especially true while using the $tanh$ activation function. 
                
    \section{Long-Short Term Memory}
    \label{sec:lstm-background}

        \emph{Long-Short Term Memory (LSTM)} \cite{hochreiterLongShortTermMemory1997} \cite{gravesLongShorttermMemory2012} are a type of \nameref{sec:rnn-background} architecture but are designed to improve upon the issues of regular RNN models as are mentioned in \nameref{par:vanishing-gradients-background} and \nameref{par:exploding-gradients-background}. 
        The main architectural improvement for LSTMs over traditional RNNs is that they have introduced \nameref{sec:lstm-memory-cell-architecture-background} to produce paths, which let gradients flow for a long duration, and thus gradients will not vanish.
        Compared to RNNs, LSTMs are better suited for learning long term dependencies in sequential or time-series data that have temporal dependence such as natural language processing, speech recognition, music generation and financial prediction.
        \begin{quote}
            Hence standard RNNs fail to learn in the presence of time lags greater than 5 – 10 discrete time steps between relevant input events and target signals. The vanishing error problem casts doubt on whether standard RNNs can indeed exhibit significant practical advantages over time window-based feedforward networks. A recent model, “Long Short-Term Memory” (LSTM), is not affected by this problem. LSTM can learn to bridge minimal time lags in excess of 1000 discrete time steps by enforcing constant error flow through “constant error carousels” (CECs) within special units, called cells. \cite{gersLearningForgetContinual2000}
        \end{quote}
        LSTMs are a type of deep learning algorithm that can be trained end-to-end, making them highly flexible and suitable for complex problems where the underlying relationships between input and output are not well understood.



        \subsection{LSTM Memory Cell Architecture}
        \label{sec:lstm-memory-cell-architecture-background}

            The LSTM architecture is comprised of a series of memory cells that store information and gate mechanisms that are used to control the flow of information into and out of the cells.
            Compared to RNN cells, they are more complex and require more computations in general.
            The introduction of \emph{self-looping} to produce paths is the main architectural improvement for LSTMs over RNNs. This additional component diminishes the problem of the vanishing gradients problem and enables gradients to flow for a long duration when extended with gate units as is described in the upcoming sections.
            \begin{figure}[h!]
                \centering
                \includegraphics[width=0.6\textwidth]{figures/lstm_memory_cell.png}
                \caption{Architecture of a memory cell and it's gate units \cite{hochreiterLongShortTermMemory1997}}
                \label{fig:architecture-of-a-memory-cell}
            \end{figure}
            LSTM memory cells are able to store information for an extended period of time compared to regular RNNs and its \emph{cell state} functions as the memory of the network.
            The architecture of a memory cell and it's gates can be seen in figure \ref{fig:architecture-of-a-memory-cell} that was taken directly from the paper \cite{hochreiterLongShortTermMemory1997}.


            \subsubsection{LSTM Gate Structure}
            \label{sec:lstm-gate-structure}

                The key building block in LSTM cells is a gate structure.
                Each memory cell has gate components that control the flow of information into and out of the cell.
                Information is either added or removed through these gates.
                The usage of gates to control the information flow allows the LSTM model to selectively remember or forget information, making it well-suited for tasks that require long-term memory.
                The gate types are as follows:

            \paragraph{Input Gate} 
            
                This gate determines the flow of new information and how much from the current time step should be added to the cell state. The input gate is implemented as a multiplicative gate which protects other units such as the cell state from perturbation by irrelevant input information.

            \paragraph{Forget Gate}

                This gate determines how much of the previously obtained information should be forgotten in order for the network to maintain a long-term memory of relevant information and discard irrelevant information.
                \begin{pabox}{Forget Gate}
                    \label{def:lstm-forget-gate-definition}
                    The forget gate is denoted by $$f_i^{(t)} = \sigma \left(b_i^f + \sum_{j} U_{i, j}^f x_j^{(t)} + \sum_{j} W_{i, j}^f h_j^{(t - 1)}\right)$$ where $t$ is the time step and $i$ is the memory cell, $x^{(t)}$ is the current input vector, $h^{t}$ is the current hidden state containing the outputs of all the LSTM cells, $b^f$ is the bias, $U^f$ the input weights and $W^f$ the recurrent weights for the forget gates.
                \end{pabox}
                
                % which sets the weight value of the forget gate between $0$ and $1$. 

                

            \paragraph{Output Gate}

                The output gate controls what information is being used for the cell state, and is sent to the network as input for the next time step. The output of the output gate is referred to as the \emph{hidden state}, denoted as $h_i^{(t)}$ and is sent to the next network layer or also used for predictions.



            \subsubsection{Sigmoid Layer and Pointwise-Multiplication}
            \label{sec:sigmoid-layer-and-pointwise-multiplication}

                Optionally, information can also be passed through by those gates via a Sigmoid layer and point-wise multiplication as can be seen in figure \ref{fig:sigmoid-layer-and-pointwise-multiplication}.
                \begin{figure}[h!]
                    \centering
                    \includegraphics[width=0.2\textwidth]{figures/sigmoid_layer.drawio.png}
                    \caption{Sigmoid Layer and Pointwise Multiplication}
                    \label{fig:sigmoid-layer-and-pointwise-multiplication}
                \end{figure}
                The Sigmoid layer is used to map the input to an output in a range of $0$ and $1$, which is used to determine how much of the information is captured while passing through the gate, and how much of the information will be retained while the pass-through.
                If the Sigmoid output is $0$, no information is kept and if the Sigmoid output is $1$, all information about the input is kept.

            \subsubsection{Back-Propagation Through Time}
            \label{sec:back-propagation-through-time}

                The LSTM architecture implements so called \emph{back-propagation through time (BPTT)}, a variant of back-propagation that allows the gradients to flow backwards through the entire sequence of inputs as can be seen in figure \ref{fig:loss-calculation-weight-update-rnn}.

            
    \subsection{LSTM Learning Process} % working title

        \paragraph{1. Forget Irrelevant History}

            The process of forgetting irrelevant history is done by the forget gates (see \nameref{sec:lstm-gate-structure}).
            This process is beneficial since not all information in a sequence is important. 

        \paragraph{2. Perform Computations and Store Relevant Information}

            The importance of sequential information is decided by the LSTM model, that either keeps information that is relevant to keep or will be discarded by the forget gates in order to be able to store new information.
            
            \begin{pabox}{Forget Gate}

                This process is mathematically represented as:

                $$g_i^{(t)} = \sigma \left(b_i^g + \sum_{j} U_{i, j}^f x_j^{(t)} + \sum_{j} W_{i, j}^g h_j^{(t - 1)}\right)$$

                The computation of new information is done via the input gates. The internal cell state is updated by the previous hidden state $h^{(t - 1)}$, the current input $x^{(t)}$ and the bias $b^{(t)}$ that are passed to the Sigmoid activation function that weights the value by transforming it between $0$ and $1$ based on how relevant the information is.
                
            \end{pabox}

        


        \paragraph{3. Self-Loop to Update Internal State}
        \label{par:self-loop-to-update-internal-state}

            Once enough data was gathered by the model, the self-loop weight $f_i^t$ is used to update the internal state of the model.

            \begin{pabox}{Internal State}
                $$s_i^{(t)} = f_i^t s_i^{t - 1} + g_i^t \sigma \left(b_i + \sum_{j} U_{i, j} x_j^{(t)} + \sum_{j} W_{i, j} h_j^{(t - 1)}\right)$$
            \end{pabox}
            First, point-wise multiplication that was calculated of the previous cell state denoted as $s_i^{(t - 1)}$  is applied. Next, the output of input gate $g_i^t$ with the computation of the biases, input and recurrent weights are calculated and then added to the self-loop weight.

        \paragraph{4. Output Gate}

            In the last step, the output is forwarded by the output gate. This output is also called \emph{hidden state}, that is sent to the next network component and also used for predictions.
            This hidden state is denoted by $h_i^t$ and has the mathematical representation:

            \begin{pabox}{Output Gate}
                $$h_i^{(t)} = \tanh \left(s_i^{(t)}\right) \sigma \left(b_i^o + \sum_{j} U_{i, j}^o x_j^{(t)} + \sum_j W_{i, j}^o h_j^{(t - 1)}\right)$$
            \end{pabox}
            The new cell state that was calculated in \nameref{par:self-loop-to-update-internal-state} is passed to the $\tanh$ activation function. Then is multiplied by the Sigmoid output of the neural network operation that was performed on the input of the current time step and the previous outputs.

    \subsection{Stacked LSTM}
    \label{sec:stacked-lstm}

        The original LSTM model is comprised of a single hidden LSTM layer and it's output is consumed by a standard feedforward output layer.
        A stacked LSTM is an extension to this model, and it can be defined as an LSTM model comprised of multiple LSTM layers.
        An LSTM layer above another layer provides a sequence output rather than a single value output to the LSTM layer below. Each layer contains of multiple memory cells and those layers are stacked onto each other, giving the name to this variant.
        The usage of stacked LSTM hidden layers is responsible for making the model deeper.
        The success of neural networks is generally attributed to the increased depth of neural networks in recent years, resulting to be applicable for a wide range of challenging prediction and optimisation problems.

        \begin{quote}
            [The success of deep neural networks] is commonly attributed to the hierarchy that is introduced due to the several layers.
            Each layer processes some part of the task we wish to solve, and passes it on to the next.
            In this sense, the DNN can be seen as a processing pipeline, in which each layer solves a part of the task before passing it on to the next, until finally the last layer provides the output. \cite{hermansTrainingAnalysingDeep2013}
        \end{quote}
        The additional hidden layers are understood to recombine the learned representation from prior layers and create new representations at high level of abstraction.
        Thus, since LSTMs operate on sequential data, the addition of layers adds levels of abstraction of input observations over time.

        \begin{quote}
            While it is not theoretically clear what is the additional power gained by the deeper architecture, it was observed empirically that deep RNNs work better than shallower ones on some tasks. \cite{goldbergPrimerNeuralNetwork2016}
            
            % In particular, Sutskever et al (2014) report that a 4-layer deep architecture was crucial in achieving good machine-translation performance in an encoder-decoder framework. Irsoy and Cardie (2014) also report improved results from moving from a one-layer-BI-RNN to an architecture with several layers. Many other works report results using layered RNN architectures, but do not explicitly compare to 1-layer RNNs. 
        \end{quote}
        While the above quote states that it is not theoretically proven as to why a deeper RNN architecture is beneficial for predictions, stacked LSTMs have become a stable technique for challenging sequence prediction problems. The stacked LSTM architecture is similar to adding additional hidden layers to the Neural Network to make it deeper. Those additional hidden layers are understood to recombine the learned representation of prior layers and use them to create new representations at a higher level of abstraction than the previous layer.
