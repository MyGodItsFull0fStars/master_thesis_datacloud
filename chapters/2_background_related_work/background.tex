\chapter{Background and Related Work}
\label{ch:background}

    \section{Monitoring}
    \label{sec:monitoring-background}

        Monitoring is the process of continuously observing and tracking a system, process, or activity in order to gather data, identify trends, and detect any deviations or issues.
        Monitoring has the purpose to ensure that the system is functioning as intended and also to identify any potential problems early on in order to mitigate them. Additionally, monitoring provides necessary information for effective decision making and problem solving.
        There are different monitoring types, including \emph{system-, environmental-, process-, performance- and security monitoring}.
        We mainly focus on \nameref{par:system-monitoring-background}, \nameref{par:process-monitoring-background} and \nameref{par:performance-monitoring-background}.
        Monitoring can be performed manually or through the use of automated tools and technologies.
        The choice of monitoring approach will depend on the specific requirements of the system or process being monitored.
        


        \paragraph{System Monitoring}
        \label{par:system-monitoring-background}

            System monitoring involves monitoring the performance and availability of computer systems, applications and networks.

        \paragraph{Process Monitoring}
        \label{par:process-monitoring-background}

        \paragraph{Performance Monitoring}
        \label{par:performance-monitoring-background}

% There are many types of monitoring, including:

%     System monitoring: This involves monitoring the performance and availability of computer systems, networks, and applications.

%     Environmental monitoring: This involves monitoring environmental parameters such as temperature, humidity, and air quality.

%     Process monitoring: This involves monitoring industrial processes to ensure that they are operating efficiently and safely.

%     Performance monitoring: This involves monitoring the performance of individuals, teams, and organizations in order to identify areas for improvement and to measure progress.

%     Security monitoring: This involves monitoring systems and networks for security threats and vulnerabilities.

% Monitoring can be performed manually or through the use of automated tools and technologies. The choice of monitoring approach will depend on the specific requirements of the system or process being monitored.

% Overall, monitoring is a critical aspect of any system or process, as it provides the information necessary for effective management, problem solving, and decision making.

    \section{Computing Continuum}
    \label{sec:computing-continuum-background}

        Placeholder


    \section{Scheduling and Adaptation}
    \label{sec:scheduling-and-adaptation-background}

    \section{Prediction of Load}
    \label{sec:prediction-of-load-background}

    \section{Recurrent Neural Network}
    \label{sec:rnn-background}

        Recurrent Neural Networks (RNNs) are a type of neural network designed to handle sequential data.
        Opposed to traditional feedforward neural networks, which receive a fixed-sized vector as input and produce a fixed-sized vector as output, 
        RNNs are able to handle sequences of variable length and produce a hidden state that summarizes information from the entire sequence.
        \begin{figure}[h!]
            \centering
            \includegraphics[scale=0.5]{figures/FNN_vs_RNN.drawio.png}
            \caption{FNN vs. RNN}
            \label{fig:fnn-vs-rnn}
        \end{figure}
        In an RNN, each unit in the network processes the input sequence one time step at a time, maintaining a hidden state that summarizes information from all previous time steps. At each time step, the hidden state is updated based on both the current input and the hidden state from the previous time step. The hidden state is then used to generate the output for that time step, and the process is repeated for each time step in the sequence.
        RNNs are used for a variety of tasks, including natural language processing, speech recognition, and video analysis. They can also be used to generate sequences, such as in text generation or music composition.

        The ability to maintain information across a sequence of inputs makes RNNs a fitting tool for modeling sequential data.
        Yet, this also results in them being prone to \nameref{par:vanishing-gradients-background} and \nameref{par:exploding-gradients-background}, that make it cumbersome to train the network effectively. 
        To address these issues, several variants of RNNs have been developed, \nameref{sec:lstm-background} networks and Gated Recurrent Units (GRUs).
        
        
        \subsection{Recurrent Cell Architecture}
        \label{sec:recurrent-cell-architecture-background}

            A traditional feed-forward neural network (FNN) is unidirectional, meaning that they have a single direction and hence cannot persist information over a time step $t$.
            Looping structures are added to a feed-forward neural network that enable the persistance of information about time-series or sequential data. 
            This is the reason RNN's are known as "recurrent" neural networks.

            As can be seen in figure \ref{fig:fnn-vs-rnn}, the RNN has an additional loop inside it to persist time-series information. The loop structure enables the RNN to apply a \emph{recurrence relation} (see \ref{sec:recurrence-relation-background}) at every time step in order to process a sequence.

            The rectangle containing the "RNN" label is defined as a "recurrent cell".
            This workflow of a single RNN cell can be seen in more detail in figure \ref{fig:single-rnn-cell}, where the previous cell state $h_{t-1}$ and the current input $x_t$ are used as the input of the recurrent cell, get combined and forwarded to the $tanh$ \emph{activation function}, which returns the output vector $\hat{y}_t$ and $h_t$ (the recurrence relation).

        \subsection{Recurrence Relation}
        \label{sec:recurrence-relation-background}

            The recurrence relation is applied at every time step to process a sequence.

            The recurrent relation seen in figures \ref{fig:fnn-vs-rnn}, \ref{fig:single-rnn-cell} is denoted as $h_t$, and is defined as $h_t = f_w(h_{t - 1}, x_t)$, where $f_w$ is a function that is parametrized by the weights, $h_{t-1}$ is the previous state and $x_t$ is the input vector at time step $t$. With the addition of $h_{t-1}$, the model is now also taking the previous time step into account when updating the current time step. 
            \begin{figure}[h!]
                \centering
                \includegraphics[scale=0.5]{figures/single_rnn_cell.drawio.png}
                \caption{Single RNN Cell}
                \label{fig:single-rnn-cell}
            \end{figure}

            The recurrence relation in a more mathematical notation is:

            $$h_t = \tanh(W^T_{hh} h_{t-1} + W^T_{xh} x_t)$$
            
            Both the input vector $x_t$ as well as the previous state $h_{t-1}$ are multiplied with the two separate weight matrices $W^T_{xh}$ and $W^T{hh}$ respectively, combined and fed to the $\tanh$ activation function. Finally, the $\tanh$ function returns the output vector $\hat{y}_t$ at time step $t$.

        \subsection{RNN Loop Unfolding}
        \label{sec:rnn-loop-unfolding-background}

            The loop unfolding will provide a description about how RNN handle sequential data at every time step.
            As can  e seen in figure \ref{fig:rnn-loop-unfolding}, the model is adding the input at every time step, and generating an output $\hat{y}_i$ for every time step.
            The weight matrices $W_{hh}$ that are used for every time step for updating the previous state are the same for every time step.
            The weight matrix $W_{xh}$ is applied to every $x_i$ and is also the same for every time step $i$.

            \begin{figure}[h!]
                \centering
                \includegraphics[width=0.90\textwidth]{figures/rnn_loop_unfolding.drawio.png}
                \caption{RNN Loop Unfolding}
                \label{fig:rnn-loop-unfolding}
            \end{figure}
            \begin{itemize}[label=\textemdash]
                \item $x_i$ denotes the input value at time step $i$.
                \item $\hat{y}_i$ denotes the output value at time step $i$.
                \item $W_{hh}$ denotes the weight matrix to update the previous state.
                \item $W_{xh}$ denotes the weight matrix that is applied to the input value at every time step.
            \end{itemize}
            The output vectors $\hat{y}_0, \hat{y}_1, \hat{y}_2, \dots, \hat{y}_t$ can be used to calculate the separate losses $L_0, L_1, L_2, \dots, L_t$ at each time step $t$.
            % This completes the forward propagation (see \ref{sec:forward-propagation}).

        \subsection{Loss Calculation and Weight Updates for RNN}
        \label{sec:loss-calculation-and-weight-updates-for-rnn-background}
        
            The training of an unfolded RNN is done through multiple time steps, as can be seen in figure \ref{fig:rnn-loop-unfolding}.
            The overall loss is defined as $L = L_0, L_1, \dots, L_t$ and is calculated from the outputs $\hat{y}_0, \hat{y}_1, \dots, \hat{y}_t$  for each time step $t$ in the \emph{forward-propagation} process.
            \begin{figure}[h!]
                \centering
                \includegraphics[width=0.6\textwidth]{figures/rnn_loss_calculation.drawio.png}
                \caption{Loss Calculation and Weight Update in RNNs}
                \label{fig:loss-calculation-weight-update-rnn}
            \end{figure}
            Then the total loss $L$ is used to propagate backwards and calculate the \emph{back-propagation} in order to update the weights of the model.
            This is shown in figure \ref{fig:loss-calculation-weight-update-rnn}, where the black arrows display the forward-propagation step that are accumulated as the total loss $L$, and then the back-propagation shown as the red arrows update the weights by using the partial derivative.

            \begin{quote}
                The central problem that back-propagation solves is the evaluation of the influence of a parameter on a function whose computation involves several elementary \emph{steps}. The solution to this problem is given by the chain rule, but back-propagation exploits the particular form of the functions used at each step (or layer) to provide an elegant and local procedure. \cite{lecunTheoreticalFrameworkBackpropagation1988}
            \end{quote}
            Back-propagation is the practice of fine-tuning the weights of a neural network based on the error rate (i.e. loss) obtained in the previous epoch. Proper tuning of the weights ensures lower error rates, making the model reliable by increasing its generalization.


        \subsection{Common Problems and Shortcomings of RNNs}
        \label{sec:shortcomings-of-rnns-background}

            Regularly experienced problems of RNNs are the exploding or vanishing gradient problems.
            \begin{quote}
                The motivation behind why they happen is that it is hard to catch long haul conditions as a result of a multiplicative angle that can be dramatically diminishing/expanding regarding the number of layers. \cite{parikhDisadvantagesRNN2021}
            \end{quote}
            While unfolding, the error gradient is calculated as the sum of all gradient errors across time steps. Therefore, the loss calculation in unfolded RNNs is also known as \emph{backpropagation through time (BPTT)}.
            Over time while calculating the error gradients the domination of the multiplicative term increases due to the chain rule application. And thus the gradients either explode or vanish.
            These problems occur when the sequence is too long and this may result in the model training with either null weights (i.e. the model won't learn while training) or exploding weights.
            
            \paragraph{Exploding Gradients}
            \label{par:exploding-gradients-background}

                The Exploding Gradients problem occurs when many of the values (i.e. weight matrices, or gradients themselves) involved in the repeated gradient computations are greater than 1. 
                If this is the case, then gradients become extremely large and optimising them becomes computationally intensive.
                To solve the Exploding Gradients problem, a process called \emph{Gradient Clipping} is applied, that scales the gradient values to smaller values less than 1.


            \paragraph{Vanishing Gradients}
            \label{par:vanishing-gradients-background}

                The Vanishing Gradients problem occurs when many of the values (i.e. weight matrices or gradients themselves) that are involved in the repeated gradient computations are too small or less than 1. Opposed to the \nameref{par:exploding-gradients-background} problem, the gradients become smaller with each repeated computation of the gradients.
                This results in the problem of \emph{long term dependency}, were smaller sequences can be remembered and the weights updated accordingly. But for longer sequences, the model will unlikely be able to yield a good prediction performance.

                There are multiple solutions to the Vanishing Gradient problem, such as changing the activation function from $\tanh$ to \emph{Rectified Linear Unit (ReLU)}. Also initialising the weights of the model can solve this problem, though tailored heuristics (such as the \emph{Xavier Weight Initialisation}) should be used to result in omitting the Vanishing Gradients problem and also increase the efficiency of the training. Another solution is to change the architecture of the neural network and adding more complex recurrent units that are mentioned in section \ref{sec:lstm-background}.

            \paragraph{Slow and Complex Training}

                Since RNNs are recurrent one of their fundamental problems is that they require a lot of time for training when compared to FNNs. Additionally, RNNs need to calibrate the previous outputs as well as current inputs into a state change function, which in turn make RNNs harder to implement and customize and more complex to train.

            \paragraph{Difficult to Process Longer Sequences}

                As already mentioned earlier, training RNNs on too long sequences is difficult without taking any measures to improve the prediction performance. This is especially true while using the $tanh$ activation function. 
                
    \section{Long-Short Term Memory}
    \label{sec:lstm-background}

        \emph{Long-Short Term Memory (LSTM)} are a type of \nameref{sec:rnn-background} architecture but are designed to improve upon the issues of regular RNN models as are mentioned in \nameref{par:vanishing-gradients-background} and \nameref{par:exploding-gradients-background}. 
        The main architectural improvement for LSTMs over traditional RNNs is that they have introduced \nameref{sec:self-looping-background} to produce paths, which let gradients flow for a long duration, and thus gradients will not vanish.
        Compared to RNNs, LSTMs are better suited for learning long term dependencies in sequential or time-series data that have temporal dependence such as natural language processing, speech recognition, music generation and financial prediction.
        LSTMs are a type of deep learning algorithm that can be trained end-to-end, making them highly flexible and suitable for complex problems where the underlying relationships between input and output are not well understood.



        \subsection{LSTM Memory Cell Architecture}
        \label{sec:lstm-memory-cell-architecture-background}

            The LSTM architecture is comprised of a series of memory cells that store information and gate mechanisms that are used to control the flow of information into and out of the cells.
            Compared to RNN cells, they are more complex and require more computations in general.
            LSTM memory cells are able to store information for an extended period of time compared to regular RNNs and its \emph{cell state} functions as the memory of the network.

            \subsubsection{LSTM Gate Structure}
            \label{sec:lstm-gate-structure}
            The key building block in LSTM cells is a gate structure.
            Each memory cell has gate components that control the flow of information into and out of the cell.
            Information is either added or removed through these gates.
            The usage of gates to control the information flow allows the LSTM model to selectively remember or forget information, making it well-suited for tasks that require long-term memory.
            The gate types are as follows:

            \paragraph{Input Gate} 
            
                This gate determines the flow of new information and how much from the current time step should be added to the cell state.

            \paragraph{Forget Gate}

                This gate determines how much of the previously obtained information should be forgotten in order for the network to maintain a long-term memory of relevant information and discard irrelevant information.
                The forget gate is denoted by $$f_i^{(t)} = \sigma \left(b_i^f + \sum_{j} U_{i, j}^f x_j^{(t)} + \sum_{j} W_{i, j}^f h_j^{(t - 1)}\right)$$, where $t$ is the time step and $i$ is the memory cell, $x^{(t)}$ is the current input vector, $h^{t}$ is the current hidden state containing the outputs of all the LSTM cells, $b^f$ is the bias, $U^f$ the input weights and $W^f$ the recurrent weights for the forget gates.
                
                % which sets the weight value of the forget gate between $0$ and $1$. 

                

            \paragraph{Output Gate}

                The output gate controls what information is being used fo the cell state, and is sent to the network as input for the next time step.





            \subsubsection{Sigmoid Layer and Pointwise-Multiplication}
            \label{sec:sigmoid-layer-and-pointwise-multiplication}
                Optionally, information can also be passed through by those gates via a Sigmoid layer and point-wise multiplication as can be seen in figure \ref{fig:sigmoid-layer-and-pointwise-multiplication}.
                \begin{figure}[h!]
                    \centering
                    \includegraphics[width=0.2\textwidth]{figures/sigmoid_layer.drawio.png}
                    \caption{Sigmoid Layer and Pointwise Multiplication}
                    \label{fig:sigmoid-layer-and-pointwise-multiplication}
                \end{figure}
                The Sigmoid layer is used to map the input to an output in a range of $0$ and $1$, which is used to determine how much of the information is captured while passing through the gate, and how much of the information will be retained while the pass-through.
                If the Sigmoid output is $0$, no information is kept and if the Sigmoid output is $1$, all information about the input is kept.

            \subsubsection{Back-Propagation Through Time}
            \label{sec:back-propagation-through-time}
                The LSTM architecture implements so called \emph{back-propagation through time (BPTT)}, a variant of back-propagation that allows the gradients to flow backwards through the entire sequence of inputs as can be seen in figure \ref{fig:loss-calculation-weight-update-rnn}.

            


        \subsection{Self-Looping}
        \label{sec:self-looping-background}

                    
            The introduction of \emph{self-looping} to produce paths is the main architectural improvement for LSTMs over RNNs. This additional component diminishes the problem of the vanishing gradients problem and enables gradients to flow for a long duration.


    \subsection{LSTM Learning Process} % working title

        \paragraph{1. Forget Irrelevant History}

            The process of forgetting irrelevant history is done by the forget gates (see \nameref{sec:lstm-gate-structure}).
            This process is beneficial since not all information in a sequence is important. 

        \paragraph{2. Perform Computations and Store Relevant Information}

            The importance of sequential information is decided by the LSTM model, that either keeps information that is relevant to keep or will be discarded by the forget gates in order to be able to store new information.
            This process is mathematically represented as:
        
            $$g_i^{(t)} = \sigma \left(b_i^g + \sum_{j} U_{i, j}^f x_j^{(t)} + \sum_{j} W_{i, j}^g h_j^{(t - 1)}\right)$$
            The computation of new information is done via the input gates. The internal cell state is updated by the previous hidden state $h^{(t - 1)}$, the current input $x^{(t)}$ and the bias $b^{(t)}$ that are passed to the Sigmoid activation function that weights the value by transforming it between $0$ and $1$ based on how relevant the information is.


        \paragraph{3. Self-Loop to Update Internal State}
        \label{par:self-loop-to-update-internal-state}

            Once enough data was gathered by the model, the self-loop weight $f_i^t$ is used to update the internal state of the model.

            $$s_i^{(t)} = f_i^t s_i^{t - 1} + g_i^t \sigma \left(b_i + \sum_{j} U_{i, j} x_j^{(t)} + \sum_{j} W_{i, j} h_j^{(t - 1)}\right)$$
            First, point-wise multiplication that was calculated of the previous cell state denoted as $s_i^{(t - 1)}$  is applied. Next, the output of input gate $g_i^t$ with the computation of the biases, input and recurrent weights are calculated and then added to the self-loop weight.

        \paragraph{4. Output Gate}

            In the last step, the output is forwarded by the output gate. This output is also called \emph{hidden state}, that is sent to the next network component and also used for predictions.
            This hidden state is denoted by $h_i^t$ and has the mathematical representation:

            $$h_i^{(t)} = \tanh \left(s_i^{(t)}\right) \sigma \left(b_i^o + \sum_{j} U_{i, j}^o x_j^{(t)} + \sum_j W_{i, j}^o h_j^{(t - 1)}\right)$$
            The new cell state that was calculated in \nameref{par:self-loop-to-update-internal-state} is passed to the $\tanh$ activation function. Then is multiplied by the Sigmoid output of the neural network operation that was performed on the input of the current time step and the previous outputs.

    \subsection{Stacked LSTM}
    \label{sec:stacked-lstm}

        The original LSTM model is comprised of a single hidden LSTM layer and it's output is consumed by a standard feedforward output layer.
        A stacked LSTM is an extension to this model, and it can be defined as an LSTM model comprised of multiple LSTM layers.
        An LSTM layer above another layer provides a sequence output rather than a single value output to the LSTM layer below. Each layer contains of multiple memory cells and those layers are stacked onto each other, giving the name to this variant.
        The usage of stacked LSTM hidden layers is responsible for making the model deeper.
        The success of neural networks is generally attributed to the increased depth of neural networks in recent years, resulting to be applicable for a wide range of challenging prediction and optimisation problems.

        \begin{quote}
            [The success of deep neural networks] is commonly attributed to the hierarchy that is introduced due to the several layers.
            Each layer processes some part of the task we wish to solve, and passes it on to the next.
            In this sense, the DNN can be seen as a processing pipeline, in which each layer solves a part of the task before passing it on to the next, until finally the last layer provides the output. \cite{hermansTrainingAnalysingDeep2013}
        \end{quote}
        The additional hidden layers are understood to recombine the learned representation from prior layers and create new representations at high level of abstraction.
        Thus, since LSTMs operate on sequential data, the addition of layers adds levels of abstraction of input observations over time.

        \begin{quote}
            While it is not theoretically clear what is the additional power gained by the deeper architecture, it was observed empirically that deep RNNs work better than shallower ones on some tasks. In particular, Sutskever et al (2014) report that a 4-layer deep architecture was crucial in achieving good machine-translation performance in an encoder-decoder framework. Irsoy and Cardie (2014) also report improved results from moving from a one-layer-BI-RNN to an architecture with several layers. Many other works report results using layered RNN architectures, but do not explicitly compare to 1-layer RNNs. \cite{goldbergPrimerNeuralNetwork2016}
        \end{quote}
        While the above quote states that it is not theoretically proven as to why a deeper RNN architecture is beneficial for predictions, stacked LSTMs have become a stable technique for challenging sequence prediction problems.
