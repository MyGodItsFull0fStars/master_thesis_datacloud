
    % --------------------------------------------------------------------
    % RELATED WORK

    % --------------------------------------------------------------------

    % Go based on the objectives or machine learning methods, research prediction....
    % Categorization of state of the art, how many papers are there for cloud, fog, or for machine learning stuff.

    % This section is for my competing approaches.

    \section{Cloud Monitoring and Execution Traces}
    \label{sec:public-cloud-provider-traces-in-available-data-related-work}

        Cloud monitoring involves the tracking of Quality of Service (QoS) parameters of a heterogeneous environment (ie. VM, storage, network and appliances), as well as the physical resources those environments reside on. Additionally, the applications that are running in those environments and the data that is stored and hosted on them.
        Data that is gathered by monitoring tools is often referred to as \emph{monitoring traces}.
        Large companies such as Google and Alibaba make some of their gathered monitoring traces public in order to enable researchers to analyse datasets of large scale cloud systems.
        These monitoring traces can be used for \emph{workload characterizations}, that is simulating a representation of various production workloads in order to use them for studies regarding scheduling or resource management strategy.

        The traces gathered by Alibaba are collected and can be accessed in a GitHub repository\footnote{Alibaba GitHub Repository: https://github.com/alibaba/clusterdata} and are part of the \emph{Alibaba Cluster Trace Program}. This program aims to help researchers and students to study the workload of different data centers. The repository is split into five sections that were collected for a specific purpose and have different time frames ranging from twelve hours to two months.
        Two sections traced the workload of machines that handled regular tasks that don't require specialised hardware. The \emph{cluster-trace-gpu-v2020} contains a dataset collected over two months of approximately 1800 machines that have 6500 GPUs in total that describe AI/ML workloads that are provided by the \emph{Alibaba Platform for Artificial Intelligence}. This dataset is mainly used for the \nameref{sec:evaluation-scenarios}. 
        Additionally, there are datasets that focus on microservices and microarchitectures. The first contains call dependencies, response times, call rates and furthermore, the second was not available at the time of conducting the evaluation of the masters thesis. Cheng et al. \cite{chengCharacterizingColocatedDatacenter2018} analysed the cluster traces of Google and Alibaba and found that warehouses have high unnecessary costs because the overall resource utilisation was between $20-40\%$. They performed a case study to characterize co-located long-running and batch job workloads on Alibaba clusters and observed patterns of overbooking, over-provisioning and poorly predicted resource usage and straggler issues (an unexpected phenomenon with a slow task processing speed performance encountered in distributed deep learning applications). 
        Fengcun et al. \cite{fengcunDeepJSJobScheduling2023} analysed the improvement of using deep reinforcement learning to schedule jobs in a cloud data center compared to hand-crafted heuristics. They used machine learning to automatically obtain a fitness calculation method, that was able to minimize the makespan directly from experience. They have shown that their deep reinforcement learning algorithm outperforms the heuristic-based job scheduling algorithms that were used in the Alibaba traces. 
        Weng et al. \cite{wengMLaaSWildWorkload2022} did a study on the workload characterization, the found temporal patterns, the GPU machine utilisation, where they suggested the usage of GPU sharing practices to fully utilise GPUs and the predictable duration for recurring tasks. They found that knowing the duration of ML task instances is key to improve the scheduling decisions but this would require specific framework support, which is not always possible. When investigating the temporal patterns, they concluded that GPU's are idle most of the time because of contention of resources such as CPU or memory. The reason for this is that instances can use \emph{spare resources} in the host machines, which can lead to utilising more resources than requested.
         
        % bridge to google and alibaba cluster

    % Cloud monitoring activity involves dynamically tracking the Quality of Service (QoS) parameters related to virtualized resources (e.g., VM, storage, network, appliances, etc.), the physical resources they share, the applications running on them and data hosted on them. Applications and resources configuration in cloud computing environment is quite challenging considering a large number of heterogeneous cloud resources. Further, considering the fact that at given point of time, there may be need to change cloud resource configuration (number of VMs, types of VMs, number of appliance instances, etc.) for meet application QoS requirements under uncertainties (resource failure, resource overload, workload spike, etc.). Hence, cloud monitoring tools can assist a cloud providers or application developers in: (i) keeping their resources and applications operating at peak efficiency, (ii) detecting variations in resource and application performance, (iii) accounting the service level agreement violations of certain QoS parameters, and (iv) tracking the leave and join operations of cloud resources due to failures and other dynamic configuration changes. In this paper, we identify and discuss the major research dimensions and design issues related to engineering cloud monitoring tools. We further discuss how the aforementioned research dimensions and design issues are handled by current academic research as well as by commercial monitoring tools.
    
    