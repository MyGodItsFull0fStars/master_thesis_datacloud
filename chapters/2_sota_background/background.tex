\chapter{Background}
\label{ch:background}

    \section{Computing Continuum}
    \label{sec:computing-continuum-background}

        Placeholder

    \section{Scheduling and Adaptation}
    \label{sec:scheduling-and-adaptation-background}

    \section{Prediction of Load}
    \label{sec:prediction-of-load-background}

    \section{Recurrent Neural Network}
    \label{sec:rnn-background}



    \section{Long-Short Term Memory}
    \label{sec:lstm-background}

        \emph{Long-Short Term Memory (LSTM)} are a type of \nameref{sec:rnn-background} but improve upon the shortcomings of regular RNN models.
        LSTMs are best suited for learning long term dependencies in sequential or time-series data.

        The introduction of \emph{self-looping} to produce paths is the main architectural improvement for LSTMs over RNNs. This additional component diminishes the problem of the vanishing gradients problem and enables gradients to flow for a long duration.