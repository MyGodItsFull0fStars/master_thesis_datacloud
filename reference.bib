@misc{agostinelliDISPIPE2023,
  title = {{{DIS-PIPE}}},
  author = {Agostinelli, Simone and Rossi, Jacopo and Marrella, Andrea and Benvenuti, Dario},
  year = {2023},
  month = feb,
  urldate = {2023-03-22},
  abstract = {Provides scalable integration of process mining techniques and AI algorithms to learn the structure of Big Data pipelines.},
  copyright = {Apache-2.0},
  howpublished = {DataCloud},
  keywords = {discovery-pipeline},
  note = {\url{https://github.com/DataCloud-project/DIS-PIPE}}
}

@book{andersonIntroductionNeuralNetworks1995,
  title = {An Introduction to Neural Networks},
  author = {Anderson, James A},
  year = {1995},
  publisher = {{MIT press}},
  isbn = {0-262-51081-2}
}

@misc{atlassianManageYourTeam,
  title = {Manage {{Your Team}}'s {{Projects From Anywhere}} | {{Trello}}},
  author = {Atlassian},
  urldate = {2023-03-23},
  abstract = {Simple, flexible, and powerful. All it takes are boards, lists, and cards to get a clear view of who's doing what and what needs to get done. Learn more in our guide for getting started.},
  howpublished = {\url{https://trello.com/}},
  file = {/Users/macbook/Zotero/storage/BMXCUJZL/trello.com.html}
}

@misc{beersWhatRegressionDefinition,
  title = {What Is {{Regression}}? {{Definition}}, {{Calculation}}, and {{Example}}},
  shorttitle = {What Is {{Regression}}?},
  author = {Beers, Brian},
  journal = {Investopedia},
  urldate = {2023-02-20},
  abstract = {Regression is a statistical measurement that attempts to determine the strength of the relationship between one dependent variable and a series of other variables.},
  howpublished = {\url{https://www.investopedia.com/terms/r/regression.asp}},
  langid = {english},
  file = {/Users/macbook/Zotero/storage/N84JSJC3/regression.html}
}

@article{botchkarevPerformanceMetricsError2018,
  title = {Performance Metrics (Error Measures) in Machine Learning Regression, Forecasting and Prognostics: {{Properties}} and Typology},
  author = {Botchkarev, Alexei},
  year = {2018},
  journal = {arXiv preprint arXiv:1809.03006},
  eprint = {1809.03006},
  archiveprefix = {arxiv}
}

@article{breimanRandomForests2001,
  title = {Random Forests},
  author = {Breiman, Leo},
  year = {2001},
  journal = {Machine learning},
  volume = {45},
  pages = {5--32},
  publisher = {{Springer}},
  issn = {0885-6125}
}

@article{chaiRootMeanSquare2014,
  title = {Root Mean Square Error ({{RMSE}}) or Mean Absolute Error ({{MAE}})?\textendash{{Arguments}} against Avoiding {{RMSE}} in the Literature},
  author = {Chai, Tianfeng and Draxler, Roland R},
  year = {2014},
  journal = {Geoscientific model development},
  volume = {7},
  number = {3},
  pages = {1247--1250},
  publisher = {{Copernicus Publications G\"ottingen, Germany}},
  issn = {1991-9603}
}

@misc{chengCharacterizingColocatedDatacenter2018,
  title = {Characterizing {{Co-located Datacenter Workloads}}: {{An Alibaba Case Study}}},
  shorttitle = {Characterizing {{Co-located Datacenter Workloads}}},
  author = {Cheng, Yue and Chai, Zheng and Anwar, Ali},
  year = {2018},
  month = aug,
  number = {arXiv:1808.02919},
  eprint = {1808.02919},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-02-21},
  abstract = {Warehouse-scale cloud datacenters co-locate workloads with different and often complementary characteristics for improved resource utilization. To better understand the challenges in managing such intricate, heterogeneous workloads while providing quality-assured resource orchestration and user experience, we analyze Alibaba's co-located workload trace, the first publicly available dataset with precise information about the category of each job. Two types of workload\textemdash long-running, user-facing, containerized production jobs, and transient, highly dynamic, non-containerized, and nonproduction batch jobs\textemdash are running on a shared cluster of 1313 machines. Our multifaceted analysis reveals insights that we believe are useful for system designers and IT practitioners working on cluster management systems.},
  archiveprefix = {arxiv},
  howpublished = {\url{http://arxiv.org/abs/1808.02919}},
  langid = {english},
  keywords = {{Computer Science - Distributed, Parallel, and Cluster Computing}},
  file = {/Users/macbook/Zotero/storage/Y7DVBV4E/Cheng et al. - 2018 - Characterizing Co-located Datacenter Workloads An.pdf}
}

@article{chenIntelligentTaskOffloading2020,
  title = {An Intelligent Task Offloading Algorithm ({{iTOA}}) for {{UAV}} Edge Computing Network},
  author = {Chen, Jienan and Chen, Siyu and Luo, Siyu and Wang, Qi and Cao, Bin and Li, Xiaoqian},
  year = {2020},
  journal = {Digital Communications and Networks},
  volume = {6},
  number = {4},
  pages = {433--443},
  publisher = {{Elsevier}},
  issn = {2352-8648}
}

@article{chenIRAFDeepReinforcement2019,
  title = {{{iRAF}}: {{A}} Deep Reinforcement Learning Approach for Collaborative Mobile Edge Computing {{IoT}} Networks},
  author = {Chen, Jienan and Chen, Siyu and Wang, Qi and Cao, Bin and Feng, Gang and Hu, Jianhao},
  year = {2019},
  journal = {IEEE Internet of Things Journal},
  volume = {6},
  number = {4},
  pages = {7011--7024},
  publisher = {{IEEE}},
  issn = {2327-4662}
}

@article{demyttenaereMeanAbsolutePercentage2016,
  title = {Mean Absolute Percentage Error for Regression Models},
  author = {De Myttenaere, Arnaud and Golden, Boris and Le Grand, B{\'e}n{\'e}dicte and Rossi, Fabrice},
  year = {2016},
  journal = {Neurocomputing},
  volume = {192},
  pages = {38--48},
  publisher = {{Elsevier}},
  issn = {0925-2312}
}

@misc{dockerDockerDocumentationOverview2023,
  title = {Docker {{Documentation Overview}}},
  author = {Docker},
  year = {2023},
  month = mar,
  journal = {Docker Documentation},
  urldate = {2023-03-28},
  abstract = {Get started with the Docker basics in this comprehensive overview, You'll learn about containers, images, and how to containerize your first application.},
  howpublished = {\url{https://docs.docker.com/get-started/}},
  langid = {english},
  file = {/Users/macbook/Zotero/storage/HPFPMSJI/get-started.html}
}

@inproceedings{doerrAdaptiveSchedulingRealtime1999,
  title = {Adaptive Scheduling for Real-Time, Embedded Information Systems},
  booktitle = {Gateway to the {{New Millennium}}. 18th {{Digital Avionics Systems Conference}}. {{Proceedings}} ({{Cat}}. {{No}}. {{99CH37033}})},
  author = {Doerr, Bryan S and Venturella, Thomas and Jha, Rakesh and Gill, Christopher D and Schmidt, Douglas C},
  year = {1999},
  volume = {1},
  pages = {2-D},
  publisher = {{IEEE}},
  isbn = {0-7803-5749-3}
}

@misc{dumitruDataCloudToolbox,
  title = {{{DataCloud Toolbox}}},
  author = {Dumitru, Roman},
  journal = {DataCloud Toolbox},
  urldate = {2023-03-22},
  abstract = {The DataCloud Toolbox provides 6 main tools (DIS-PIPE, DEF-PIPE, SIM-PIPE, ADA-PIPE, R-MARKET and DEP-PIPE) as shown in the figure below. Each tool has its own GitHub repository and may consist of different tool components having their separate component repository on GitHub.},
  howpublished = {\url{https://datacloud-project.github.io/toolbox/}},
  file = {/Users/macbook/Zotero/storage/4YXDGVLB/toolbox.html}
}

@misc{dumitruProjectProjectDataCloud,
  title = {About the {{Project}} | {{Project}} | {{DataCloud Project}}},
  author = {Dumitru, Roman},
  urldate = {2023-03-22},
  howpublished = {\url{https://datacloudproject.eu/project/about-the-project}},
  file = {/Users/macbook/Zotero/storage/R7J6JHJG/about-the-project.html}
}

@misc{EdgeComputingVs,
  title = {Edge {{Computing}} vs. {{Fog Computing}}: 10 {{Key Comparisons}} |},
  shorttitle = {Edge {{Computing}} vs. {{Fog Computing}}},
  urldate = {2023-01-23},
  abstract = {While edge computing brings the computers closer to the source of data, cloud computing makes advanced technology available over the internet for a fixed, recurring fee.},
  howpublished = {\url{https://www.spiceworks.com/tech/cloud/articles/edge-vs-fog-computing/}},
  langid = {american},
  file = {/Users/macbook/Zotero/storage/JI327NNE/edge-vs-fog-computing.html}
}

@misc{FastAPI,
  title = {{{FastAPI}}},
  urldate = {2023-05-27},
  howpublished = {\url{https://fastapi.tiangolo.com/lo/}},
  file = {/Users/macbook/Zotero/storage/62MZLC4V/lo.html}
}

@misc{fawcettDataScienceMinutes,
  title = {Data {{Science}} in 5 {{Minutes}}: {{What}} Is {{One Hot Encoding}}?},
  shorttitle = {Data {{Science}} in 5 {{Minutes}}},
  author = {Fawcett, Amanda},
  journal = {Educative: Interactive Courses for Software Developers},
  urldate = {2023-02-18},
  abstract = {One hot encoding is the process of converting categorical data variables into numerical values. Learn how to one hot encode in Pandas and Sklearn.},
  howpublished = {\url{https://www.educative.io/blog/one-hot-encoding}},
  langid = {english},
  file = {/Users/macbook/Zotero/storage/E8ABDG7V/one-hot-encoding.html}
}

@misc{fengcunDeepJSJobScheduling2023,
  title = {{{DeepJS}}: {{Job Scheduling Based}} on {{Deep Reinforcement Learning}} in {{Cloud Data Center}}},
  author = {Fengcun, Li and Bo, Hu},
  year = {2023},
  month = feb,
  urldate = {2023-02-21},
  abstract = {CloudSimPy: Datacenter job scheduling simulation framework},
  copyright = {MIT},
  howpublished = {\url{https://github.com/FengcunLi/CloudSimPy/blob/c103672f51d6617707501f05548a7df6090cdca5/playground/paper/F0049-4.19.pdf}}
}

@misc{FullyConnectedLayer,
  title = {Fully Connected Layer},
  urldate = {2023-02-25},
  abstract = {The simplest of NN's},
  howpublished = {\url{https://www.fastaireference.com/tabular-data/fully-connected-layer}},
  langid = {english},
  file = {/Users/macbook/Zotero/storage/HPRHQ7JK/fully-connected-layer.html}
}

@article{gersLearningForgetContinual2000,
  title = {Learning to {{Forget}}: {{Continual Prediction}} with {{LSTM}}},
  shorttitle = {Learning to {{Forget}}},
  author = {Gers, Felix A. and Schmidhuber, J{\"u}rgen and Cummins, Fred},
  year = {2000},
  month = oct,
  journal = {Neural Computation},
  volume = {12},
  number = {10},
  pages = {2451--2471},
  issn = {0899-7667},
  doi = {10.1162/089976600300015015},
  urldate = {2023-02-19},
  abstract = {Long short-term memory (LSTM; Hochreiter \&amp; Schmidhuber, 1997) can solve numerous tasks not solvable by previous learning algorithms for recurrent neural networks (RNNs). We identify a weakness of LSTM networks processing continual input streams that are not a priori segmented into subsequences with explicitly marked ends at which the network's internal state could be reset. Without resets, the state may grow indefinitely and eventually cause the network to break down. Our remedy is a novel, adaptive ``forget gate'' that enables an LSTM cell to learn to reset itself at appropriate times, thus releasing internal resources. We review illustrative benchmark problems on which standard LSTM outperforms other RNN algorithms. All algorithms (including LSTM) fail to solve continual versions of these problems. LSTM with forget gates, however, easily solves them, and in an elegant way.},
  note = {\url{https://doi.org/10.1162/089976600300015015}},
  file = {/Users/macbook/Zotero/storage/8WGG32K2/Learning-to-Forget-Continual-Prediction-with-LSTM.html}
}

@article{goldbergPrimerNeuralNetwork2016,
  title = {A Primer on Neural Network Models for Natural Language Processing},
  author = {Goldberg, Yoav},
  year = {2016},
  journal = {Journal of Artificial Intelligence Research},
  volume = {57},
  pages = {345--420},
  issn = {1076-9757}
}

@book{goodfellowDeepLearning2016,
  title = {Deep Learning},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  publisher = {{MIT press}},
  isbn = {0-262-33737-1}
}

@article{gravesLongShorttermMemory2012,
  title = {Long Short-Term Memory},
  author = {Graves, Alex and Graves, Alex},
  year = {2012},
  journal = {Supervised sequence labelling with recurrent neural networks},
  pages = {37--45},
  publisher = {{Springer}},
  issn = {3642247962}
}

@article{hartiganAlgorithm136Kmeans1979,
  title = {Algorithm {{AS}} 136: {{A}} k-Means Clustering Algorithm},
  author = {Hartigan, John A and Wong, Manchek A},
  year = {1979},
  journal = {Journal of the royal statistical society. series c (applied statistics)},
  volume = {28},
  number = {1},
  pages = {100--108},
  publisher = {{JSTOR}},
  issn = {0035-9254}
}

@inproceedings{hermansTrainingAnalysingDeep2013,
  title = {Training and {{Analysing Deep Recurrent Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Hermans, Michiel and Schrauwen, Benjamin},
  year = {2013},
  volume = {26},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-02-13},
  abstract = {Time series often have a temporal hierarchy, with information that is spread out over multiple time scales. Common recurrent neural networks, however, do not explicitly accommodate such a hierarchy, and most research on them has been focusing on training algorithms rather than on their basic architecture. In this pa- per we study the effect of a hierarchy of recurrent neural networks on processing time series. Here, each layer is a recurrent network which receives the hidden state of the previous layer as input. This architecture allows us to perform hi- erarchical processing on difficult temporal tasks, and more naturally capture the structure of time series. We show that they reach state-of-the-art performance for recurrent networks in character-level language modelling when trained with sim- ple stochastic gradient descent. We also offer an analysis of the different emergent time scales.},
  note = {\url{https://papers.nips.cc/paper/2013/hash/1ff8a7b5dc7a7d1f0ed65aaa29c04b1e-Abstract.html}},
  file = {/Users/macbook/Zotero/storage/GN9EHVBU/Hermans and Schrauwen - 2013 - Training and Analysing Deep Recurrent Neural Netwo.pdf}
}

@article{hochreiterLongShortTermMemory1997,
  title = {Long {{Short-Term Memory}}},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = {1997},
  month = nov,
  journal = {Neural Computation},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  issn = {0899-7667},
  doi = {10.1162/neco.1997.9.8.1735},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  file = {/Users/macbook/Zotero/storage/VGN6MQQ9/6795963.html}
}

@article{huMobilityawareEdgeCaching2018,
  title = {Mobility-Aware Edge Caching and Computing in Vehicle Networks: {{A}} Deep Reinforcement Learning},
  author = {Hu, Rose Qingyang},
  year = {2018},
  journal = {IEEE Transactions on Vehicular Technology},
  volume = {67},
  number = {11},
  pages = {10190--10203},
  publisher = {{IEEE}},
  issn = {0018-9545}
}

@article{husseinEnhancementPerformanceRandom2021,
  title = {Enhancement Performance of Random Forest Algorithm via One Hot Encoding for {{IoT IDS}}},
  author = {Hussein, Adil Yousef and Falcarin, Paolo and Sadiq, Ahmed T},
  year = {2021},
  journal = {Periodicals of Engineering and Natural Sciences},
  volume = {9},
  number = {3},
  pages = {579--591},
  issn = {2303-4521}
}

@misc{inkawhichSavingLoadingModels,
  title = {Saving and {{Loading Models}} \textemdash{} {{PyTorch Tutorials}} 2.0.0+cu117 Documentation},
  author = {Inkawhich, Matthew},
  urldate = {2023-04-26},
  howpublished = {\url{https://pytorch.org/tutorials/beginner/saving_loading_models.html#export-load-model-in-torchscript-format}},
  file = {/Users/macbook/Zotero/storage/4UFG6MS2/saving_loading_models.html}
}

@article{jamesMonteCarloTheory1980,
  title = {Monte {{Carlo}} Theory and Practice},
  author = {James, Frederick},
  year = {1980},
  journal = {Reports on progress in Physics},
  volume = {43},
  number = {9},
  pages = {1145},
  publisher = {{IOP Publishing}},
  issn = {0034-4885}
}

@misc{jetbrainsWhatAreDomainSpecific,
  title = {What Are {{Domain-Specific Languages}} ({{DSL}}) | {{MPS}} by {{JetBrains}}},
  author = {JetBrains},
  journal = {JetBrains},
  urldate = {2023-03-24},
  abstract = {The major goal of MPS is to allow extending programming languages.},
  howpublished = {\url{https://www.jetbrains.com/mps/concepts/domain-specific-languages/}},
  langid = {english},
  file = {/Users/macbook/Zotero/storage/PIK4XCDH/domain-specific-languages.html}
}

@article{jiangEdgeEnhancedGANRemote2019,
  title = {Edge-{{Enhanced GAN}} for {{Remote Sensing Image Superresolution}}},
  author = {Jiang, Kui and Wang, Zhongyuan and Yi, Peng and Wang, Guangcheng and Lu, Tao and Jiang, Junjun},
  year = {2019},
  month = aug,
  journal = {IEEE Transactions on Geoscience and Remote Sensing},
  volume = {57},
  number = {8},
  pages = {5799--5812},
  issn = {1558-0644},
  doi = {10.1109/TGRS.2019.2902431},
  abstract = {The current superresolution (SR) methods based on deep learning have shown remarkable comparative advantages but remain unsatisfactory in recovering the high-frequency edge details of the images in noise-contaminated imaging conditions, e.g., remote sensing satellite imaging. In this paper, we propose a generative adversarial network (GAN)-based edge-enhancement network (EEGAN) for robust satellite image SR reconstruction along with the adversarial learning strategy that is insensitive to noise. In particular, EEGAN consists of two main subnetworks: an ultradense subnetwork (UDSN) and an edge-enhancement subnetwork (EESN). In UDSN, a group of 2-D dense blocks is assembled for feature extraction and to obtain an intermediate high-resolution result that looks sharp but is eroded with artifacts and noises as previous GAN-based methods do. Then, EESN is constructed to extract and enhance the image contours by purifying the noise-contaminated components with mask processing. The recovered intermediate image and enhanced edges can be combined to generate the result that enjoys high credibility and clear contents. Extensive experiments on Kaggle Open Source Data set, Jilin-1 video satellite images, and Digitalglobe show superior reconstruction performance compared to the state-of-the-art SR approaches.},
  keywords = {Adversarial learning,dense connection,edge enhancement,Feature extraction,Gallium nitride,Generative adversarial networks,Image edge detection,Image reconstruction,Image resolution,remote sensing imagery,Satellites,superresolution},
  file = {/Users/macbook/Zotero/storage/WF4T9XWY/Jiang et al. - 2019 - Edge-Enhanced GAN for Remote Sensing Image Superre.pdf;/Users/macbook/Zotero/storage/4RLRMQBL/8677274.html}
}

@misc{keycloakDocumentationKeycloak,
  title = {Documentation - {{Keycloak}}},
  author = {KeyCloak},
  urldate = {2023-03-21},
  howpublished = {\url{https://www.keycloak.org/documentation}},
  file = {/Users/macbook/Zotero/storage/TF3EPQ7J/documentation.html}
}

@inproceedings{kimovskiBigDataPipeline2022,
  title = {Big {{Data Pipeline Scheduling}} and {{Adaptation}} on the {{Computing Continuum}}},
  booktitle = {2022 {{IEEE}} 46th {{Annual Computers}}, {{Software}}, and {{Applications Conference}} ({{COMPSAC}})},
  author = {Kimovski, Dragi and Bauer, Christian and Mehran, Narges and Prodan, Radu},
  year = {2022},
  month = jun,
  pages = {1153--1158},
  issn = {0730-3157},
  doi = {10.1109/COMPSAC54236.2022.00181},
  abstract = {The Computing Continuum, covering Cloud, Fog, and Edge systems, promises to provide on-demand resource-as-a-service for Internet applications with diverse requirements, ranging from extremely low latency to high-performance processing. However, eminent challenges in automating the resources man-agement of Big Data pipelines across the Computing Continuum remain. The resource management and adaptation for Big Data pipelines across the Computing Continuum require significant research effort, as the current data processing pipelines are dynamic. In contrast, traditional resource management strategies are static, leading to inefficient pipeline scheduling and overly complex process deployment. To address these needs, we propose in this work a scheduling and adaptation approach implemented as a software tool to lower the technological barriers to the management of Big Data pipelines over the Computing Continuum. The approach separates the static scheduling from the run-time execution, em-powering domain experts with little infrastructure and software knowledge to take an active part in the Big Data pipeline adaptation. We conduct a feasibility study using a digital healthcare use case to validate our approach. We illustrate concrete scenarios supported by demonstrating how the scheduling and adaptation tool and its implementation automate the management of the lifecycle of a remote patient monitoring, treatment, and care pipeline.},
  keywords = {Adaptation,Big Data,Computing Continuum,Electronic healthcare,Fog and Edge computing,Patient monitoring,Processor scheduling,Resource management,Resources management,Scheduling,Software as a service,Task analysis},
  file = {/Users/macbook/Zotero/storage/9CLQYDNC/9842650.html}
}

@article{koksoyMultiresponseRobustDesign2006,
  title = {Multiresponse Robust Design: {{Mean}} Square Error ({{MSE}}) Criterion},
  author = {K{\"o}ksoy, Onur},
  year = {2006},
  journal = {Applied Mathematics and Computation},
  volume = {175},
  number = {2},
  pages = {1716--1729},
  publisher = {{Elsevier}},
  issn = {0096-3003}
}

@misc{konginc.IntroductionInsomnia,
  title = {Introduction to {{Insomnia}}},
  author = {Kong Inc.},
  journal = {Welcome to Insomnia Docs},
  urldate = {2023-03-22},
  abstract = {Insomnia is an open source desktop application that takes the pain out of interacting with and designing, debugging, and testing APIs. Insomnia combines an easy-to-use interface with advanced functionality like authentication helpers, code generation, and environment variables.},
  howpublished = {\url{https://docs.insomnia.rest/insomnia/get-started}},
  langid = {american},
  file = {/Users/macbook/Zotero/storage/4MPEU3LL/docs.insomnia.rest.html}
}

@misc{konginc.UnitTestingInsomnia,
  title = {Unit {{Testing}} | {{Insomnia Docs}}},
  author = {Kong Inc.},
  urldate = {2023-03-23},
  howpublished = {\url{https://docs.insomnia.rest/insomnia/unit-testing}},
  langid = {american},
  file = {/Users/macbook/Zotero/storage/JAK7RWCL/unit-testing.html}
}

@article{kotsiantisDecisionTreesRecent2013,
  title = {Decision Trees: A Recent Overview},
  author = {Kotsiantis, Sotiris B},
  year = {2013},
  journal = {Artificial Intelligence Review},
  volume = {39},
  pages = {261--283},
  publisher = {{Springer}},
  issn = {0269-2821}
}

@article{kreinovichHowEstimateForecasting2014,
  title = {How to Estimate Forecasting Quality: {{A}} System-Motivated Derivation of Symmetric Mean Absolute Percentage Error ({{SMAPE}}) and Other Similar Characteristics},
  author = {Kreinovich, Vladik and Nguyen, Hung T and Ouncharoen, Rujira},
  year = {2014}
}

@inproceedings{lecunTheoreticalFrameworkBackpropagation1988,
  title = {A Theoretical Framework for Back-Propagation},
  booktitle = {Proceedings of the 1988 Connectionist Models Summer School},
  author = {LeCun, Yann and Touresky, D and Hinton, G and Sejnowski, T},
  year = {1988},
  volume = {1},
  pages = {21--28},
  file = {/Users/macbook/Zotero/storage/HWLJYU6G/A-Theoretical-Framework-for-Back-Propagation.pdf}
}

@misc{ledakisDEPPIPE,
  title = {{{DEP-PIPE}}},
  author = {Ledakis, Giannis and Plakas, Ioannis and Elves{\ae}ter, Brian and Theodosiou, Konstantinos},
  urldate = {2023-03-22},
  abstract = {The aim of this project is to develop an autonomous Datacloud Microservice that provides the essential tools and operations for handling Steps, Chunks, Pipelines and Pipeline deployments.},
  howpublished = {\url{https://github.com/DataCloud-project/DEP-PIPE-Pipeline-Deployment-Controller}},
  file = {/Users/macbook/Zotero/storage/5KBAQY88/DEP-PIPE-Pipeline-Deployment-Controller.html}
}

@article{likasGlobalKmeansClustering2003,
  title = {The Global K-Means Clustering Algorithm},
  author = {Likas, Aristidis and Vlassis, Nikos and Verbeek, Jakob J},
  year = {2003},
  journal = {Pattern recognition},
  volume = {36},
  number = {2},
  pages = {451--461},
  publisher = {{Elsevier}},
  issn = {0031-3203}
}

@misc{mehranADAPIPE2023,
  title = {{{ADA-PIPE}}},
  author = {Mehran, Narges and Bauer, Christian},
  year = {2023},
  month = feb,
  urldate = {2023-03-22},
  abstract = {Provides a data-aware algorithm for adaptable provisioning of resources across the Computing Continuum.},
  copyright = {Apache-2.0},
  howpublished = {DataCloud},
  keywords = {adaptation-pipeline},
  note = {\url{https://github.com/DataCloud-project/ADA-PIPE}}
}

@misc{mehranDSLDEFPIPEExample2023,
  title = {{{DSL-DEF-PIPE Example}}},
  author = {Mehran, Narges and Bauer, Christian},
  year = {2023},
  month = feb,
  urldate = {2023-03-22},
  abstract = {Provides a data-aware algorithm for adaptable provisioning of resources across the Computing Continuum.},
  copyright = {Apache-2.0},
  howpublished = {DataCloud},
  note = {\url{https://github.com/DataCloud-project/ADA-PIPE/blob/04ed7959883afbf5ba536a5688d7e61e30ff4be4/ImportFrom-DEF-PIPE/tellu.dsl}}
}

@inproceedings{mehranMatchingbasedSchedulingAsynchronous2022,
  title = {Matching-Based {{Scheduling}} of {{Asynchronous Data Processing Workflows}} on the {{Computing Continuum}}},
  booktitle = {2022 {{IEEE International Conference}} on {{Cluster Computing}} ({{CLUSTER}})},
  author = {Mehran, Narges and Samani, Zahra Najafabadi and Kimovski, Dragi and Prodan, Radu},
  year = {2022},
  pages = {58--70},
  publisher = {{IEEE}},
  isbn = {1-66549-856-0}
}

@misc{microsoftSkypeStayConnected,
  title = {Skype | {{Stay}} Connected with Free Video Calls Worldwide},
  author = {Microsoft},
  urldate = {2023-03-23},
  abstract = {Keep in touch with free video chat, messaging \& affordable international calls. Create instant\,online\,video calls\,with one click, no download required.},
  howpublished = {\url{https://www.skype.com/en/}},
  langid = {english},
  file = {/Users/macbook/Zotero/storage/CYWV3YNL/en.html}
}

@misc{mitrovicDEFPIPEFrontend2022,
  title = {{{DEF-PIPE Frontend}}},
  author = {Mitrovic, Vlado and Layegh, Amirhossein and Elves{\ae}ter, Brian and Tahmasebi, Shirin},
  year = {2022},
  month = feb,
  urldate = {2023-03-22},
  abstract = {Graphic tool for designing data pipelines and tranforming them to DSL.},
  copyright = {Apache-2.0},
  howpublished = {DataCloud},
  keywords = {definition-pipeline},
  note = {\url{https://github.com/DataCloud-project/DEF-PIPE-Frontend}}
}

@misc{netdataGettingStartedLearn2023,
  title = {Getting Started | {{Learn Netdata}}},
  author = {Netdata},
  year = {2023},
  month = mar,
  urldate = {2023-03-14},
  abstract = {Present netdata in a nutshell. The section includes everything needed to have a basic Netdata setup running, including overview of configuration, installing an agent, and cloud basics.},
  howpublished = {\url{https://learn.netdata.cloud/docs/getting-started/}},
  langid = {english},
  file = {/Users/macbook/Zotero/storage/3YSUAZYF/getting-started.html}
}

@inproceedings{ngoContextualbanditAnomalyDetection2020,
  title = {Contextual-Bandit Anomaly Detection for {{IoT}} Data in Distributed Hierarchical Edge Computing},
  booktitle = {2020 {{IEEE}} 40th {{International Conference}} on {{Distributed Computing Systems}} ({{ICDCS}})},
  author = {Ngo, Mao V and Luo, Tie and Chaouchi, Hakima and Quek, Tony QS},
  year = {2020},
  pages = {1227--1230},
  publisher = {{IEEE}},
  isbn = {1-72817-002-8}
}

@misc{nikolovSIMPIPE,
  title = {{{SIM-PIPE}}},
  author = {Nikolov, Nikolay and Pultier, Antoine and Thomas, Aleena and Elves{\ae}ter, Brian},
  urldate = {2023-03-22},
  howpublished = {\url{https://github.com/DataCloud-project/SIM-PIPE}},
  file = {/Users/macbook/Zotero/storage/Q3LM4TJN/SIM-PIPE.html}
}

@misc{NmapNetworkMapper,
  title = {Nmap: The {{Network Mapper}} - {{Free Security Scanner}}},
  journal = {Nmap: Discover your network},
  urldate = {2023-03-22},
  abstract = {Nmap ("Network Mapper") is a free and open source utility for network discovery and security auditing. Many systems and network administrators also find it useful for tasks such as network inventory, managing service upgrade schedules, and monitoring host or service uptime.},
  howpublished = {\url{https://nmap.org/}},
  file = {/Users/macbook/Zotero/storage/HF9HVS2Q/nmap.org.html}
}

@inproceedings{orenSOLOSearchOnline2021,
  title = {{{SOLO}}: Search Online, Learn Offline for Combinatorial Optimization Problems},
  booktitle = {Proceedings of the {{International Symposium}} on {{Combinatorial Search}}},
  author = {Oren, Joel and Ross, Chana and Lefarov, Maksym and Richter, Felix and Taitler, Ayal and Feldman, Zohar and Di Castro, Dotan and Daniel, Christian},
  year = {2021},
  volume = {12},
  pages = {97--105}
}

@misc{orenSOLOSearchOnline2021a,
  title = {{{SOLO}}: {{Search Online}}, {{Learn Offline}} for {{Combinatorial Optimization Problems}}},
  shorttitle = {{{SOLO}}},
  author = {Oren, Joel and Ross, Chana and Lefarov, Maksym and Richter, Felix and Taitler, Ayal and Feldman, Zohar and Daniel, Christian and Di Castro, Dotan},
  year = {2021},
  month = may,
  number = {arXiv:2104.01646},
  eprint = {2104.01646},
  primaryclass = {cs, math},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2104.01646},
  urldate = {2023-04-28},
  abstract = {We study combinatorial problems with real world applications such as machine scheduling, routing, and assignment. We propose a method that combines Reinforcement Learning (RL) and planning. This method can equally be applied to both the offline, as well as online, variants of the combinatorial problem, in which the problem components (e.g., jobs in scheduling problems) are not known in advance, but rather arrive during the decision-making process. Our solution is quite generic, scalable, and leverages distributional knowledge of the problem parameters. We frame the solution process as an MDP, and take a Deep Q-Learning approach wherein states are represented as graphs, thereby allowing our trained policies to deal with arbitrary changes in a principled manner. Though learned policies work well in expectation, small deviations can have substantial negative effects in combinatorial settings. We mitigate these drawbacks by employing our graph-convolutional policies as non-optimal heuristics in a compatible search algorithm, Monte Carlo Tree Search, to significantly improve overall performance. We demonstrate our method on two problems: Machine Scheduling and Capacitated Vehicle Routing. We show that our method outperforms custom-tailored mathematical solvers, state of the art learning-based algorithms, and common heuristics, both in computation time and performance.},
  archiveprefix = {arxiv},
  howpublished = {\url{http://arxiv.org/abs/2104.01646}},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control},
  file = {/Users/macbook/Zotero/storage/P7MCFZWL/Oren et al. - 2021 - SOLO Search Online, Learn Offline for Combinatori.pdf;/Users/macbook/Zotero/storage/QAWXWMPL/2104.html}
}

@article{orrOptimalTaskScheduling2021,
  title = {Optimal Task Scheduling for Partially Heterogeneous Systems},
  author = {Orr, Michael and Sinnen, Oliver},
  year = {2021},
  month = oct,
  journal = {Parallel Computing},
  volume = {107},
  pages = {102815},
  issn = {0167-8191},
  doi = {10.1016/j.parco.2021.102815},
  urldate = {2023-04-23},
  abstract = {Task scheduling with communication delays is a strongly NP-hard problem. Previous attempts at finding optimal solutions to this problem have used branch-and-bound state\textendash space search, with promising results. However, the scheduling model used assumes a target system with fully homogeneous processors, which is unrealistic for many real world systems for which task scheduling might be performed. This paper presents an extension to the Allocation-Ordering (AO) state\textendash space model for task scheduling which allows a system with related heterogeneous processors to be modeled, and optimal schedules on such a system to be found. Of particular note, the distinct allocation phase allows this model to efficiently adapt to partially heterogeneous systems, in which subsets of the processors are identical to each other, which significantly helps to reduce the search space. An extensive experimental evaluation shows that the introduction of heterogeneity certainly increases the difficulty of the problem. However, many problem instances solvable using homogeneous processors remain solvable with a heterogeneous target system, made possible by the significant benefit of this model in considering partial heterogeneity.},
  langid = {english},
  keywords = {Branch-and-bound,Combinatorial optimization,Heterogeneous processors,Parallel computing,Task scheduling},
  note = {\url{https://www.sciencedirect.com/science/article/pii/S0167819121000636}},
  file = {/Users/macbook/Zotero/storage/89WKUZWR/S0167819121000636.html}
}

@misc{pandasPandasDocumentationPandas,
  title = {Pandas Documentation \textemdash{} Pandas 1.5.3 Documentation},
  author = {Pandas},
  journal = {pandas documentation},
  urldate = {2023-03-14},
  abstract = {pandas is an open-source, BSD-licenced library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.},
  howpublished = {\url{https://pandas.pydata.org/docs/}},
  langid = {english},
  file = {/Users/macbook/Zotero/storage/TUYWKZG3/docs.html}
}

@misc{parikhDisadvantagesRNN2021,
  title = {Disadvantages of {{RNN}}},
  author = {Parikh, Dishant},
  year = {2021},
  month = jan,
  journal = {OpenGenus IQ: Computing Expertise \& Legacy},
  urldate = {2023-02-10},
  abstract = {We have explored the disadvantages of RNN in depth. Recurrent Neural Networks (or RNNs) are the first of their kind neural networks that can help in analyzing and learning sequences of data rather than just instance-based learning.},
  howpublished = {\url{https://iq.opengenus.org/disadvantages-of-rnn/}},
  langid = {english},
  file = {/Users/macbook/Zotero/storage/GIXNQF9X/disadvantages-of-rnn.html}
}

@article{patelHybridCNNLSTMModel2022,
  title = {A Hybrid {{CNN-LSTM}} Model for Predicting Server Load in Cloud Computing},
  author = {Patel, Eva and Kushwaha, Dharmender Singh},
  year = {2022},
  journal = {The Journal of Supercomputing},
  volume = {78},
  number = {8},
  pages = {1--30},
  publisher = {{Springer}},
  issn = {0920-8542}
}

@misc{patelWhatFeatureEngineering2021,
  title = {What Is {{Feature Engineering}} \textemdash{} {{Importance}}, {{Tools}} and {{Techniques}} for {{Machine Learning}}},
  author = {Patel, Harshil},
  year = {2021},
  month = sep,
  journal = {Medium},
  urldate = {2023-02-28},
  abstract = {Feature engineering is the process of selecting, manipulating, and transforming raw data into features that can be used in supervised\ldots},
  howpublished = {\url{https://towardsdatascience.com/what-is-feature-engineering-importance-tools-and-techniques-for-machine-learning-2080b0269f10}},
  langid = {english},
  file = {/Users/macbook/Zotero/storage/2CX6D3H5/what-is-feature-engineering-importance-tools-and-techniques-for-machine-learning-2080b0269f10.html}
}

@misc{phpmyadminBringingMySQLWeb,
  title = {Bringing {{MySQL}} to the Web},
  author = {{phpMyAdmin}},
  journal = {phpMyAdmin},
  urldate = {2023-03-22},
  abstract = {phpMyAdmin is a free software tool written in PHP, intended to handle the administration of MySQL over the Web. phpMyAdmin supports a wide range of operations on MySQL and MariaDB. Frequently used operations (managing databases, tables, columns, relations, indexes, users, permissions, etc) can be performed via the user interface, while you still have the ability to directly execute any SQL statement.},
  howpublished = {\url{https://www.phpmyadmin.net/}},
  langid = {english},
  file = {/Users/macbook/Zotero/storage/XXXRNFHJ/www.phpmyadmin.net.html}
}

@misc{prometheusOverviewPrometheus,
  title = {Overview | {{Prometheus}}},
  author = {Prometheus},
  journal = {Overview - What is Prometheus?},
  urldate = {2023-03-14},
  abstract = {An open-source monitoring system with a dimensional data model, flexible query language, efficient time series database and modern alerting approach.},
  howpublished = {\url{https://prometheus.io/docs/introduction/overview/}},
  langid = {english},
  file = {/Users/macbook/Zotero/storage/4SZDJYE3/overview.html}
}

@misc{redhatWhatRESTAPI,
  title = {What Is a {{REST API}}?},
  author = {RedHat},
  journal = {What is a REST API?},
  urldate = {2023-03-21},
  abstract = {A REST API (also known as RESTful API) is an application programming interface that conforms to the constraints of REST architecture. REST stands for representational state transfer.},
  howpublished = {\url{https://www.redhat.com/en/topics/api/what-is-a-rest-api}},
  langid = {english},
  file = {/Users/macbook/Zotero/storage/J5XDZAZT/what-is-a-rest-api.html}
}

@misc{richDeepMindAIReduces2016,
  title = {{{DeepMind AI}} Reduces Energy Used for Cooling {{Google}} Data Centers by 40\%},
  author = {Rich, Evans and Gao, Jim},
  year = {2016},
  month = jul,
  journal = {Google},
  urldate = {2023-02-16},
  abstract = {In any large scale energy-consuming environment, this would be a huge improvement. Given how sophisticated Google's data centers are already, it's a phenomenal step forward.},
  howpublished = {\url{https://blog.google/outreach-initiatives/environment/deepmind-ai-reduces-energy-used-for/}},
  langid = {american},
  file = {/Users/macbook/Zotero/storage/VVRMQM9I/deepmind-ai-reduces-energy-used-for.html}
}

@article{ritchieCOGreenhouseGas2020,
  title = {{{CO}}{$_2$} and {{Greenhouse Gas Emissions}}},
  author = {Ritchie, Hannah and Roser, Max and Rosado, Pablo},
  year = {2020},
  month = may,
  journal = {Our World in Data},
  urldate = {2023-04-25},
  abstract = {How much CO2 does the world emit? Which countries emit the most?},
  note = {\url{https://ourworldindata.org/co2-emissions}},
  file = {/Users/macbook/Zotero/storage/29BFNCRN/co2-emissions.html}
}

@misc{rouseHyperparameter2022,
  title = {Hyperparameter},
  author = {Rouse, Margaret},
  year = {2022},
  month = aug,
  journal = {Techopedia},
  urldate = {2023-04-20},
  abstract = {This definition explains the meaning of Hyperparameter and why it matters.},
  howpublished = {\url{https://www.techopedia.com/definition/34625/hyperparameter-ml-hyperparameter}},
  langid = {american},
  file = {/Users/macbook/Zotero/storage/TRVRDTTW/hyperparameter-ml-hyperparameter.html}
}

@misc{sakLongShortTermMemory2014,
  title = {Long {{Short-Term Memory Based Recurrent Neural Network Architectures}} for {{Large Vocabulary Speech Recognition}}},
  author = {Sak, Ha{\c s}im and Senior, Andrew and Beaufays, Fran{\c c}oise},
  year = {2014},
  month = feb,
  number = {arXiv:1402.1128},
  eprint = {1402.1128},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1402.1128},
  urldate = {2023-02-19},
  abstract = {Long Short-Term Memory (LSTM) is a recurrent neural network (RNN) architecture that has been designed to address the vanishing and exploding gradient problems of conventional RNNs. Unlike feedforward neural networks, RNNs have cyclic connections making them powerful for modeling sequences. They have been successfully used for sequence labeling and sequence prediction tasks, such as handwriting recognition, language modeling, phonetic labeling of acoustic frames. However, in contrast to the deep neural networks, the use of RNNs in speech recognition has been limited to phone recognition in small scale tasks. In this paper, we present novel LSTM based RNN architectures which make more effective use of model parameters to train acoustic models for large vocabulary speech recognition. We train and compare LSTM, RNN and DNN models at various numbers of parameters and configurations. We show that LSTM models converge quickly and give state of the art speech recognition performance for relatively small sized models.},
  archiveprefix = {arxiv},
  howpublished = {\url{http://arxiv.org/abs/1402.1128}},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/macbook/Zotero/storage/PYWCB7Z8/Sak et al. - 2014 - Long Short-Term Memory Based Recurrent Neural Netw.pdf;/Users/macbook/Zotero/storage/RLB29XDA/1402.html}
}

@misc{senguptaRMARKET,
  title = {R-{{MARKET}}},
  author = {Sengupta, Souvik and Djari, Aimen},
  urldate = {2023-03-22},
  abstract = {R-MARKET provides a decentralised trusted marketplace for resources (software appliances and hardware devices) spread across the Computing Continuum. The tool leverages hybrid permission and permissionless blockchain based on the Ethereum technology to federate, search and provision available resources for deploying big data pipelines. In addition, the tool maintains the history of transactions, the reputation of resource providers and consumers and executes payments for services as well as compensations for SLO violations. R-MARKET acts as an intermediary between the cloud resources from the Computing Continuum, ADA-PIPE and DEP-PIPE. In the below the figure shows the high-level architecture of the tool: the resources (Workers) are grouped in Worker Pools and listed in R-MARKET by their providers. R-MARKET provides ADA-PIPE with an API for querying resources that match a semantic description. Based on the returned list of Work Orders, ADA-PIPE signs and transmits a Request Order to the PoCo smart contracts, which will perform a series of checks, provision the Workers and hand their control over to DEP-PIPE. R-MARKET mixes two sets of interfaces: the low-level smart contracts APIs are used for critical operations (e.g., sending Request Orders), while high-level REST APIs are used for off-chain operations (e.g., querying the Order Book).},
  howpublished = {\url{https://github.com/DataCloud-project/R-MARKET}},
  file = {/Users/macbook/Zotero/storage/JC7KU9T5/R-MARKET.html}
}

@article{shapiEnergyConsumptionPrediction2021,
  title = {Energy Consumption Prediction by Using Machine Learning for Smart Building: {{Case}} Study in {{Malaysia}}},
  shorttitle = {Energy Consumption Prediction by Using Machine Learning for Smart Building},
  author = {Shapi, Mel Keytingan M. and Ramli, Nor Azuana and Awalin, Lilik J.},
  year = {2021},
  month = mar,
  journal = {Developments in the Built Environment},
  volume = {5},
  pages = {100037},
  issn = {2666-1659},
  doi = {10.1016/j.dibe.2020.100037},
  urldate = {2023-02-16},
  abstract = {Building Energy Management System (BEMS) has been a substantial topic nowadays due to its importance in reducing energy wastage. However, the performance of one of BEMS applications which is energy consumption prediction has been stagnant due to problems such as low prediction accuracy. Thus, this research aims to address the problems by developing a predictive model for energy consumption in Microsoft Azure cloud-based machine learning platform. Three methodologies which are Support Vector Machine, Artificial Neural Network, and k-Nearest Neighbour are proposed for the algorithm of the predictive model. Focusing on real-life application in Malaysia, two tenants from a commercial building are taken as a case study. The data collected is analysed and pre-processed before it is used for model training and testing. The performance of each of the methods is compared based on RMSE, NRMSE, and MAPE metrics. The experimentation shows that each tenant's energy consumption has different distribution characteristics.},
  langid = {english},
  keywords = {Building energy management system,Energy consumption,Machine learning,Microsoft Azure,Prediction},
  note = {\url{https://www.sciencedirect.com/science/article/pii/S266616592030034X}},
  file = {/Users/macbook/Zotero/storage/DPQY7E69/Shapi et al. - 2021 - Energy consumption prediction by using machine lea.pdf;/Users/macbook/Zotero/storage/Q8NP4AQT/S266616592030034X.html}
}

@article{stoopComplexitySchedulingPractice1996,
  title = {The Complexity of Scheduling in Practice},
  author = {Stoop, Paul P.M. and Wiers, Vincent C.S.},
  year = {1996},
  month = jan,
  journal = {International Journal of Operations \& Production Management},
  volume = {16},
  number = {10},
  pages = {37--53},
  publisher = {{MCB UP Ltd}},
  issn = {0144-3577},
  doi = {10.1108/01443579610130682},
  urldate = {2023-02-20},
  abstract = {Successful implementations of scheduling techniques in practice are scarce. Not only do daily disturbances lead to a gap between theory and practice, but also the extent to which a scheduling technique can adequately model the processes on the shopfloor, and the extent to which the optimization goal of a technique matches the organizational goal are not great enough. Further, the schedulers' actions may play an important role in the fulfilment of the generated schedules. The organizational structure with its different responsibilities and conflicting goals may also result in the poor performance of scheduling techniques. Besides these, there is the problem of measuring the quality of a schedule. Discusses the causes for these gaps and illustrates the solutions to improve scheduling by way of a case study.},
  keywords = {Decision making,Performance measurement,Production control,Production scheduling,Shopfloor},
  note = {\url{https://doi.org/10.1108/01443579610130682}},
  file = {/Users/macbook/Zotero/storage/Y49R6P67/Stoop and Wiers - 1996 - The complexity of scheduling in practice.pdf}
}

@misc{swaggerAPIDocumentationDesign,
  title = {{{API Documentation}} \& {{Design Tools}} for {{Teams}} | {{Swagger}}},
  author = {Swagger},
  journal = {API Development for Everyone},
  urldate = {2023-05-26},
  howpublished = {\url{https://swagger.io/}},
  file = {/Users/macbook/Zotero/storage/NRSN4LA4/swagger.io.html}
}

@misc{the-linux-foundationKubernetesDocumentationGetting,
  title = {Kubernetes {{Documentation}} / {{Getting}} Started},
  author = {{The-Linux-Foundation} and Kubernetes},
  journal = {Kubernetes},
  urldate = {2023-03-28},
  abstract = {Production-Grade Container Orchestration},
  howpublished = {\url{https://kubernetes.io/docs/setup/}},
  langid = {english},
  file = {/Users/macbook/Zotero/storage/IYAW4RUK/setup.html}
}

@misc{the-linux-foundationPyTorch,
  title = {{{PyTorch}}},
  author = {{The-Linux-Foundation}},
  journal = {Get Started},
  urldate = {2023-03-14},
  abstract = {An open source machine learning framework that accelerates the path from research prototyping to production deployment.},
  howpublished = {\url{https://www.pytorch.org}},
  langid = {english},
  file = {/Users/macbook/Zotero/storage/K8VPZHNR/locally.html}
}

@inproceedings{thonglekImprovingResourceUtilization2019,
  title = {Improving {{Resource Utilization}} in {{Data Centers}} Using an {{LSTM-based Prediction Model}}},
  booktitle = {2019 {{IEEE International Conference}} on {{Cluster Computing}} ({{CLUSTER}})},
  author = {Thonglek, Kundjanasith and Ichikawa, Kohei and Takahashi, Keichi and Iida, Hajimu and Nakasan, Chawanat},
  year = {2019},
  month = sep,
  pages = {1--8},
  issn = {2168-9253},
  doi = {10.1109/CLUSTER.2019.8891022},
  abstract = {Data centers are centralized facilities where computing and networking hardware are aggregated to handle large amounts of data and computation. In a data center, computing resources such as CPU and memory are usually managed by a resource manager. The resource manager accepts resource requests from users and allocates resources to their applications. A commonly known problem in resource management is that users often request more resources than their applications actually use. This leads to the degradation of overall resource utilization in a data center. This paper aims to improve resource utilization in data centers by predicting the required resource for each application. We designed and implemented a neural network model based on Long Short-Term Memory (LSTM) to predict more efficient resource allocation for a job based on historical data. Our model has two LSTM layers each of which learns the relationship between: (1) allocation and usage, and (2) CPU and memory. We used Googles cluster-usage trace, which contains a trace of resource allocation and usage for each job executed on a Google data center, to train our neural network. Googles cluster scheduler simulator was used to evaluate our proposed method. Our simulation indicated that the proposed method improved the CPU utilization and memory utilization by 10.71\% and 47.36\%, respectively, compared to a conventional resource manager. Moreover, we discovered that increasing the memory cell size of our LSTM model improves the accuracy of the prediction in return for longer training time.},
  keywords = {Computer architecture,Computing Resources,Data centers,Google,Long Short-Term Memory,Microprocessors,Neural networks,Predictive models,Resource management,Resource Management,Resource Utilization},
  file = {/Users/macbook/Zotero/storage/TRXGC6L4/8891022.html}
}

@article{topcuogluPerformanceeffectiveLowcomplexityTask2002,
  title = {Performance-Effective and Low-Complexity Task Scheduling for Heterogeneous Computing},
  author = {Topcuoglu, H. and Hariri, S. and Wu, Min-You},
  year = {2002},
  month = mar,
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  volume = {13},
  number = {3},
  pages = {260--274},
  issn = {1558-2183},
  doi = {10.1109/71.993206},
  abstract = {Efficient application scheduling is critical for achieving high performance in heterogeneous computing environments. The application scheduling problem has been shown to be NP-complete in general cases as well as in several restricted cases. Because of its key importance, this problem has been extensively studied and various algorithms have been proposed in the literature which are mainly for systems with homogeneous processors. Although there are a few algorithms in the literature for heterogeneous processors, they usually require significantly high scheduling costs and they may not deliver good quality schedules with lower costs. In this paper, we present two novel scheduling algorithms for a bounded number of heterogeneous processors with an objective to simultaneously meet high performance and fast scheduling time, which are called the Heterogeneous Earliest-Finish-Time (HEFT) algorithm and the Critical-Path-on-a-Processor (CPOP) algorithm. The HEFT algorithm selects the task with the highest upward rank value at each step and assigns the selected task to the processor, which minimizes its earliest finish time with an insertion-based approach. On the other hand, the CPOP algorithm uses the summation of upward and downward rank values for prioritizing tasks. Another difference is in the processor selection phase, which schedules the critical tasks onto the processor that minimizes the total execution time of the critical tasks. In order to provide a robust and unbiased comparison with the related work, a parametric graph generator was designed to generate weighted directed acyclic graphs with various characteristics. The comparison study, based on both randomly generated graphs and the graphs of some real applications, shows that our scheduling algorithms significantly surpass previous approaches in terms of both quality and cost of schedules, which are mainly presented with schedule length ratio, speedup, frequency of best results, and average scheduling time metrics.},
  keywords = {Processor scheduling},
  file = {/Users/macbook/Zotero/storage/A2LEJH7W/993206.html}
}

@article{tuliStartStragglerPrediction2021,
  title = {Start: {{Straggler}} Prediction and Mitigation for Cloud Computing Environments Using Encoder Lstm Networks},
  author = {Tuli, Shreshth and Gill, Sukhpal Singh and Garraghan, Peter and Buyya, Rajkumar and Casale, Giuliano and Jennings, Nick},
  year = {2021},
  journal = {IEEE Transactions on Services Computing},
  publisher = {{IEEE}},
  issn = {1939-1374}
}

@misc{vanooteghemWhatHyperscaleData2023,
  title = {What Is a Hyperscale Data Center?},
  author = {Van Ooteghem, Karel},
  year = {2023},
  month = jan,
  journal = {Parallels Remote Application Server Blog - Application virtualization, mobility and VDI},
  urldate = {2023-04-23},
  abstract = {Learn about what a hyperscale data center is and how it can benefit your business. Parallels RAS supports multiple CSPs which handle hyperscaling.},
  howpublished = {\url{https://www.parallels.com/blogs/ras/hyperscale-data-center/}},
  langid = {american},
  file = {/Users/macbook/Zotero/storage/2K6INYFK/hyperscale-data-center.html}
}

@misc{wattsWhatCodeRefactoring2018,
  title = {What Is {{Code Refactoring}}? {{How Refactoring Resolves Technical Debt}}},
  shorttitle = {What Is {{Code Refactoring}}?},
  author = {Watts, Stephen and Kidd, Chrissy},
  year = {2018},
  month = mar,
  journal = {BMC Blogs},
  urldate = {2023-03-23},
  howpublished = {\url{https://www.bmc.com/blogs/code-refactoring-explained/}},
  langid = {american},
  file = {/Users/macbook/Zotero/storage/GUALET3P/code-refactoring-explained.html}
}

@book{weisbergAppliedLinearRegression2005,
  title = {Applied Linear Regression},
  author = {Weisberg, Sanford},
  year = {2005},
  volume = {528},
  publisher = {{John Wiley \& Sons}},
  isbn = {0-471-70408-3}
}

@misc{WelcomeFlaskFlask,
  title = {Welcome to {{Flask}} \textemdash{} {{Flask Documentation}} (2.2.x)},
  journal = {Flask Documentation},
  urldate = {2023-03-21},
  howpublished = {\url{https://flask.palletsprojects.com/en/2.2.x/}},
  file = {/Users/macbook/Zotero/storage/6FG9BWRK/2.2.x.html}
}

@misc{wengAlibabaClusterTrace2023,
  title = {Alibaba {{Cluster Trace Program}}},
  author = {Weng, Qizhen and Ding, Haiyang},
  year = {2023},
  month = apr,
  urldate = {2023-04-08},
  abstract = {cluster data collected from production clusters in Alibaba for cluster management research},
  howpublished = {Alibaba},
  keywords = {dataset},
  note = {\url{https://github.com/alibaba/clusterdata}}
}

@misc{wengClusterdataClustertracegpuv2020Master,
  title = {Clusterdata/Cluster-Trace-Gpu-V2020 at Master {$\cdot$} Alibaba/Clusterdata},
  author = {Weng, Qizhen and Xiao, Wencong and Yu, Yinghao and Wang, Wei and Wang, Cheng and He, Jian},
  journal = {Cluster Trace GPU v2020},
  urldate = {2023-04-08},
  howpublished = {\url{https://github.com/alibaba/clusterdata/tree/master/cluster-trace-gpu-v2020}},
  file = {/Users/macbook/Zotero/storage/P8XBGU66/cluster-trace-gpu-v2020.html}
}

@article{wengMLaaSWildWorkload,
  title = {{{MLaaS}} in the {{Wild}}: {{Workload Analysis}} and {{Scheduling}} in {{Large-Scale Heterogeneous GPU Clusters}}},
  author = {Weng, Qizhen and Xiao, Wencong and Yu, Yinghao and Wang, Wei and Wang, Cheng and He, Jian and Li, Yong and Zhang, Liping and Lin, Wei and Ding, Yu},
  abstract = {With the sustained technological advances in machine learning (ML) and the availability of massive datasets recently, tech companies are deploying large ML-as-a-Service (MLaaS) clouds, often with heterogeneous GPUs, to provision a host of ML applications. However, running diverse ML workloads in heterogeneous GPU clusters raises a number of challenges. In this paper, we present a characterization study of a two-month workload trace collected from a production MLaaS cluster with over 6,000 GPUs in Alibaba. We explain the challenges posed to cluster scheduling, including the low GPU utilization, the long queueing delays, the presence of hard-to-schedule tasks demanding high-end GPUs with picky scheduling requirements, the imbalance load across heterogeneous machines, and the potential bottleneck on CPUs. We describe our current solutions and call for further investigations into the challenges that remain open to address. We have released the trace for public access, which is the most comprehensive in terms of the workloads and cluster scale.},
  langid = {english},
  file = {/Users/macbook/Zotero/storage/2BN3765S/Weng et al. - MLaaS in the Wild Workload Analysis and Schedulin.pdf}
}

@inproceedings{wengMLaaSWildWorkload2022,
  title = {{{MLaaS}} in the Wild: {{Workload}} Analysis and Scheduling in {{Large-Scale}} Heterogeneous {{GPU}} Clusters},
  booktitle = {19th {{USENIX Symposium}} on {{Networked Systems Design}} and {{Implementation}} ({{NSDI}} 22)},
  author = {Weng, Qizhen and Xiao, Wencong and Yu, Yinghao and Wang, Wei and Wang, Cheng and He, Jian and Li, Yong and Zhang, Liping and Lin, Wei and Ding, Yu},
  year = {2022},
  pages = {945--960},
  publisher = {{USENIX Association}}
}

@article{wengMLaaSWildWorkloada,
  title = {{{MLaaS}} in the {{Wild}}: {{Workload Analysis}} and {{Scheduling}} in {{Large-Scale Heterogeneous GPU Clusters}}},
  author = {Weng, Qizhen and Xiao, Wencong and Yu, Yinghao and Wang, Wei and Wang, Cheng and He, Jian and Li, Yong and Zhang, Liping and Lin, Wei and Ding, Yu},
  abstract = {With the sustained technological advances in machine learning (ML) and the availability of massive datasets recently, tech companies are deploying large ML-as-a-Service (MLaaS) clouds, often with heterogeneous GPUs, to provision a host of ML applications. However, running diverse ML workloads in heterogeneous GPU clusters raises a number of challenges. In this paper, we present a characterization study of a two-month workload trace collected from a production MLaaS cluster with over 6,000 GPUs in Alibaba. We explain the challenges posed to cluster scheduling, including the low GPU utilization, the long queueing delays, the presence of hard-to-schedule tasks demanding high-end GPUs with picky scheduling requirements, the imbalance load across heterogeneous machines, and the potential bottleneck on CPUs. We describe our current solutions and call for further investigations into the challenges that remain open to address. We have released the trace for public access, which is the most comprehensive in terms of the workloads and cluster scale.},
  langid = {english},
  file = {/Users/macbook/Zotero/storage/UP3L2IMI/Weng et al. - MLaaS in the Wild Workload Analysis and Schedulin.pdf}
}

@article{wengMLaaSWildWorkloadb,
  title = {{{MLaaS}} in the {{Wild}}: {{Workload Analysis}} and {{Scheduling}} in {{Large-Scale Heterogeneous GPU Clusters}}},
  author = {Weng, Qizhen and Xiao, Wencong and Yu, Yinghao and Wang, Wei and Wang, Cheng and He, Jian and Li, Yong and Zhang, Liping and Lin, Wei and Ding, Yu},
  abstract = {With the sustained technological advances in machine learning (ML) and the availability of massive datasets recently, tech companies are deploying large ML-as-a-Service (MLaaS) clouds, often with heterogeneous GPUs, to provision a host of ML applications. However, running diverse ML workloads in heterogeneous GPU clusters raises a number of challenges. In this paper, we present a characterization study of a two-month workload trace collected from a production MLaaS cluster with over 6,000 GPUs in Alibaba. We explain the challenges posed to cluster scheduling, including the low GPU utilization, the long queueing delays, the presence of hard-to-schedule tasks demanding high-end GPUs with picky scheduling requirements, the imbalance load across heterogeneous machines, and the potential bottleneck on CPUs. We describe our current solutions and call for further investigations into the challenges that remain open to address. We have released the trace for public access, which is the most comprehensive in terms of the workloads and cluster scale.},
  langid = {english},
  file = {/Users/macbook/Zotero/storage/2WHGMU7Z/Weng et al. - MLaaS in the Wild Workload Analysis and Schedulin.pdf}
}

@misc{zhangArchitecturalComplexityMeasures2016,
  title = {Architectural {{Complexity Measures}} of {{Recurrent Neural Networks}}},
  author = {Zhang, Saizheng and Wu, Yuhuai and Che, Tong and Lin, Zhouhan and Memisevic, Roland and Salakhutdinov, Ruslan and Bengio, Yoshua},
  year = {2016},
  month = nov,
  number = {arXiv:1602.08210},
  eprint = {1602.08210},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1602.08210},
  urldate = {2022-11-10},
  abstract = {In this paper, we systematically analyze the connecting architectures of recurrent neural networks (RNNs). Our main contribution is twofold: first, we present a rigorous graph-theoretic framework describing the connecting architectures of RNNs in general. Second, we propose three architecture complexity measures of RNNs: (a) the recurrent depth, which captures the RNN's over-time nonlinear complexity, (b) the feedforward depth, which captures the local input-output nonlinearity (similar to the "depth" in feedforward neural networks (FNNs)), and (c) the recurrent skip coefficient which captures how rapidly the information propagates over time. We rigorously prove each measure's existence and computability. Our experimental results show that RNNs might benefit from larger recurrent depth and feedforward depth. We further demonstrate that increasing recurrent skip coefficient offers performance boosts on long term dependency problems.},
  archiveprefix = {arxiv},
  howpublished = {\url{http://arxiv.org/abs/1602.08210}},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/macbook/Zotero/storage/IDG46J4F/Zhang et al. - 2016 - Architectural Complexity Measures of Recurrent Neu.pdf;/Users/macbook/Zotero/storage/63GS7YN6/1602.html}
}
