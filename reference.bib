@book{andersonIntroductionNeuralNetworks1995,
  title = {An Introduction to Neural Networks},
  author = {Anderson, James A},
  date = {1995},
  publisher = {{MIT press}},
  isbn = {0-262-51081-2}
}

@online{beersWhatRegressionDefinition,
  title = {What Is {{Regression}}? {{Definition}}, {{Calculation}}, and {{Example}}},
  shorttitle = {What Is {{Regression}}?},
  author = {Beers, Brian},
  url = {https://www.investopedia.com/terms/r/regression.asp},
  urldate = {2023-02-20},
  abstract = {Regression is a statistical measurement that attempts to determine the strength of the relationship between one dependent variable and a series of other variables.},
  langid = {english},
  organization = {{Investopedia}},
  note = {Available at \href{https://www.investopedia.com/terms/r/regression.asp}{https://www.investopedia.com/terms/r/regression.asp}},
  file = {/Users/macbook/Zotero/storage/N84JSJC3/regression.html}
}

@unpublished{botchkarevPerformanceMetricsError2018,
  title = {Performance Metrics (Error Measures) in Machine Learning Regression, Forecasting and Prognostics: {{Properties}} and Typology},
  author = {Botchkarev, Alexei},
  date = {2018},
  shortjournal = {arXiv preprint arXiv:1809.03006},
  eprint = {1809.03006},
  eprinttype = {arxiv},
  archiveprefix = {arXiv}
}

@article{breimanRandomForests2001,
  title = {Random Forests},
  author = {Breiman, Leo},
  date = {2001},
  journaltitle = {Machine learning},
  volume = {45},
  pages = {5--32},
  publisher = {{Springer}},
  issn = {0885-6125}
}

@article{chaiRootMeanSquare2014,
  title = {Root Mean Square Error ({{RMSE}}) or Mean Absolute Error ({{MAE}})?–{{Arguments}} against Avoiding {{RMSE}} in the Literature},
  author = {Chai, Tianfeng and Draxler, Roland R},
  date = {2014},
  journaltitle = {Geoscientific model development},
  volume = {7},
  number = {3},
  pages = {1247--1250},
  publisher = {{Copernicus Publications Göttingen, Germany}},
  issn = {1991-9603}
}

@article{demyttenaereMeanAbsolutePercentage2016,
  title = {Mean Absolute Percentage Error for Regression Models},
  author = {De Myttenaere, Arnaud and Golden, Boris and Le Grand, Bénédicte and Rossi, Fabrice},
  date = {2016},
  journaltitle = {Neurocomputing},
  volume = {192},
  pages = {38--48},
  publisher = {{Elsevier}},
  issn = {0925-2312}
}

@online{EdgeComputingVs,
  title = {Edge {{Computing}} vs. {{Fog Computing}}: 10 {{Key Comparisons}} |},
  shorttitle = {Edge {{Computing}} vs. {{Fog Computing}}},
  url = {https://www.spiceworks.com/tech/cloud/articles/edge-vs-fog-computing/},
  urldate = {2023-01-23},
  abstract = {While edge computing brings the computers closer to the source of data, cloud computing makes advanced technology available over the internet for a fixed, recurring fee.},
  langid = {american},
  file = {/Users/macbook/Zotero/storage/JI327NNE/edge-vs-fog-computing.html}
}

@online{fawcettDataScienceMinutes,
  title = {Data {{Science}} in 5 {{Minutes}}: {{What}} Is {{One Hot Encoding}}?},
  shorttitle = {Data {{Science}} in 5 {{Minutes}}},
  author = {Fawcett, Amanda},
  url = {https://www.educative.io/blog/one-hot-encoding},
  urldate = {2023-02-18},
  abstract = {One hot encoding is the process of converting categorical data variables into numerical values. Learn how to one hot encode in Pandas and Sklearn.},
  langid = {english},
  organization = {{Educative: Interactive Courses for Software Developers}},
  note = {Available at \href{https://www.educative.io/blog/one-hot-encoding}{https://www.educative.io/blog/one-hot-encoding}},
  file = {/Users/macbook/Zotero/storage/E8ABDG7V/one-hot-encoding.html}
}

@article{gersLearningForgetContinual2000,
  title = {Learning to {{Forget}}: {{Continual Prediction}} with {{LSTM}}},
  shorttitle = {Learning to {{Forget}}},
  author = {Gers, Felix A. and Schmidhuber, Jürgen and Cummins, Fred},
  date = {2000-10-01},
  journaltitle = {Neural Computation},
  volume = {12},
  number = {10},
  pages = {2451--2471},
  issn = {0899-7667},
  doi = {10.1162/089976600300015015},
  url = {https://doi.org/10.1162/089976600300015015},
  urldate = {2023-02-19},
  abstract = {Long short-term memory (LSTM; Hochreiter \&amp; Schmidhuber, 1997) can solve numerous tasks not solvable by previous learning algorithms for recurrent neural networks (RNNs). We identify a weakness of LSTM networks processing continual input streams that are not a priori segmented into subsequences with explicitly marked ends at which the network's internal state could be reset. Without resets, the state may grow indefinitely and eventually cause the network to break down. Our remedy is a novel, adaptive “forget gate” that enables an LSTM cell to learn to reset itself at appropriate times, thus releasing internal resources. We review illustrative benchmark problems on which standard LSTM outperforms other RNN algorithms. All algorithms (including LSTM) fail to solve continual versions of these problems. LSTM with forget gates, however, easily solves them, and in an elegant way.},
  file = {/Users/macbook/Zotero/storage/8WGG32K2/Learning-to-Forget-Continual-Prediction-with-LSTM.html}
}

@article{goldbergPrimerNeuralNetwork2016,
  title = {A Primer on Neural Network Models for Natural Language Processing},
  author = {Goldberg, Yoav},
  date = {2016},
  journaltitle = {Journal of Artificial Intelligence Research},
  volume = {57},
  pages = {345--420},
  issn = {1076-9757}
}

@article{gravesLongShorttermMemory2012,
  title = {Long Short-Term Memory},
  author = {Graves, Alex and Graves, Alex},
  date = {2012},
  journaltitle = {Supervised sequence labelling with recurrent neural networks},
  pages = {37--45},
  publisher = {{Springer}},
  issn = {3642247962}
}

@article{hartiganAlgorithm136Kmeans1979,
  title = {Algorithm {{AS}} 136: {{A}} k-Means Clustering Algorithm},
  author = {Hartigan, John A and Wong, Manchek A},
  date = {1979},
  journaltitle = {Journal of the royal statistical society. series c (applied statistics)},
  volume = {28},
  number = {1},
  pages = {100--108},
  publisher = {{JSTOR}},
  issn = {0035-9254}
}

@inproceedings{hermansTrainingAnalysingDeep2013,
  title = {Training and {{Analysing Deep Recurrent Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Hermans, Michiel and Schrauwen, Benjamin},
  date = {2013},
  volume = {26},
  publisher = {{Curran Associates, Inc.}},
  url = {https://papers.nips.cc/paper/2013/hash/1ff8a7b5dc7a7d1f0ed65aaa29c04b1e-Abstract.html},
  urldate = {2023-02-13},
  abstract = {Time series often have a temporal hierarchy, with information that is spread out over multiple time scales. Common recurrent neural networks, however, do not explicitly accommodate such a hierarchy, and most research on them has been focusing on training algorithms rather than on their basic architecture. In this pa- per we study the effect of a hierarchy of recurrent neural networks on processing time series. Here, each layer is a recurrent network which receives the hidden state of the previous layer as input. This architecture allows us to perform hi- erarchical processing on difficult temporal tasks, and more naturally capture the structure of time series. We show that they reach state-of-the-art performance for recurrent networks in character-level language modelling when trained with sim- ple stochastic gradient descent. We also offer an analysis of the different emergent time scales.},
  file = {/Users/macbook/Zotero/storage/GN9EHVBU/Hermans and Schrauwen - 2013 - Training and Analysing Deep Recurrent Neural Netwo.pdf}
}

@article{hochreiterLongShortTermMemory1997,
  title = {Long {{Short-Term Memory}}},
  author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
  date = {1997-11},
  journaltitle = {Neural Computation},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  issn = {0899-7667},
  doi = {10.1162/neco.1997.9.8.1735},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  eventtitle = {Neural {{Computation}}},
  file = {/Users/macbook/Zotero/storage/VGN6MQQ9/6795963.html}
}

@article{husseinEnhancementPerformanceRandom2021,
  title = {Enhancement Performance of Random Forest Algorithm via One Hot Encoding for {{IoT IDS}}},
  author = {Hussein, Adil Yousef and Falcarin, Paolo and Sadiq, Ahmed T},
  date = {2021},
  journaltitle = {Periodicals of Engineering and Natural Sciences},
  volume = {9},
  number = {3},
  pages = {579--591},
  issn = {2303-4521}
}

@article{jamesMonteCarloTheory1980,
  title = {Monte {{Carlo}} Theory and Practice},
  author = {James, Frederick},
  date = {1980},
  journaltitle = {Reports on progress in Physics},
  volume = {43},
  number = {9},
  pages = {1145},
  publisher = {{IOP Publishing}},
  issn = {0034-4885}
}

@article{jiangEdgeEnhancedGANRemote2019,
  title = {Edge-{{Enhanced GAN}} for {{Remote Sensing Image Superresolution}}},
  author = {Jiang, Kui and Wang, Zhongyuan and Yi, Peng and Wang, Guangcheng and Lu, Tao and Jiang, Junjun},
  date = {2019-08},
  journaltitle = {IEEE Transactions on Geoscience and Remote Sensing},
  volume = {57},
  number = {8},
  pages = {5799--5812},
  issn = {1558-0644},
  doi = {10.1109/TGRS.2019.2902431},
  abstract = {The current superresolution (SR) methods based on deep learning have shown remarkable comparative advantages but remain unsatisfactory in recovering the high-frequency edge details of the images in noise-contaminated imaging conditions, e.g., remote sensing satellite imaging. In this paper, we propose a generative adversarial network (GAN)-based edge-enhancement network (EEGAN) for robust satellite image SR reconstruction along with the adversarial learning strategy that is insensitive to noise. In particular, EEGAN consists of two main subnetworks: an ultradense subnetwork (UDSN) and an edge-enhancement subnetwork (EESN). In UDSN, a group of 2-D dense blocks is assembled for feature extraction and to obtain an intermediate high-resolution result that looks sharp but is eroded with artifacts and noises as previous GAN-based methods do. Then, EESN is constructed to extract and enhance the image contours by purifying the noise-contaminated components with mask processing. The recovered intermediate image and enhanced edges can be combined to generate the result that enjoys high credibility and clear contents. Extensive experiments on Kaggle Open Source Data set, Jilin-1 video satellite images, and Digitalglobe show superior reconstruction performance compared to the state-of-the-art SR approaches.},
  eventtitle = {{{IEEE Transactions}} on {{Geoscience}} and {{Remote Sensing}}},
  keywords = {Adversarial learning,dense connection,edge enhancement,Feature extraction,Gallium nitride,Generative adversarial networks,Image edge detection,Image reconstruction,Image resolution,remote sensing imagery,Satellites,superresolution},
  file = {/Users/macbook/Zotero/storage/WF4T9XWY/Jiang et al. - 2019 - Edge-Enhanced GAN for Remote Sensing Image Superre.pdf;/Users/macbook/Zotero/storage/4RLRMQBL/8677274.html}
}

@inproceedings{kimovskiBigDataPipeline2022,
  title = {Big {{Data Pipeline Scheduling}} and {{Adaptation}} on the {{Computing Continuum}}},
  booktitle = {2022 {{IEEE}} 46th {{Annual Computers}}, {{Software}}, and {{Applications Conference}} ({{COMPSAC}})},
  author = {Kimovski, Dragi and Bauer, Christian and Mehran, Narges and Prodan, Radu},
  date = {2022-06},
  pages = {1153--1158},
  issn = {0730-3157},
  doi = {10.1109/COMPSAC54236.2022.00181},
  abstract = {The Computing Continuum, covering Cloud, Fog, and Edge systems, promises to provide on-demand resource-as-a-service for Internet applications with diverse requirements, ranging from extremely low latency to high-performance processing. However, eminent challenges in automating the resources man-agement of Big Data pipelines across the Computing Continuum remain. The resource management and adaptation for Big Data pipelines across the Computing Continuum require significant research effort, as the current data processing pipelines are dynamic. In contrast, traditional resource management strategies are static, leading to inefficient pipeline scheduling and overly complex process deployment. To address these needs, we propose in this work a scheduling and adaptation approach implemented as a software tool to lower the technological barriers to the management of Big Data pipelines over the Computing Continuum. The approach separates the static scheduling from the run-time execution, em-powering domain experts with little infrastructure and software knowledge to take an active part in the Big Data pipeline adaptation. We conduct a feasibility study using a digital healthcare use case to validate our approach. We illustrate concrete scenarios supported by demonstrating how the scheduling and adaptation tool and its implementation automate the management of the lifecycle of a remote patient monitoring, treatment, and care pipeline.},
  eventtitle = {2022 {{IEEE}} 46th {{Annual Computers}}, {{Software}}, and {{Applications Conference}} ({{COMPSAC}})},
  keywords = {Adaptation,Big Data,Computing Continuum,Electronic healthcare,Fog and Edge computing,Patient monitoring,Processor scheduling,Resource management,Resources management,Scheduling,Software as a service,Task analysis},
  file = {/Users/macbook/Zotero/storage/9CLQYDNC/9842650.html}
}

@article{koksoyMultiresponseRobustDesign2006,
  title = {Multiresponse Robust Design: {{Mean}} Square Error ({{MSE}}) Criterion},
  author = {Köksoy, Onur},
  date = {2006},
  journaltitle = {Applied Mathematics and Computation},
  volume = {175},
  number = {2},
  pages = {1716--1729},
  publisher = {{Elsevier}},
  issn = {0096-3003}
}

@article{kotsiantisDecisionTreesRecent2013,
  title = {Decision Trees: A Recent Overview},
  author = {Kotsiantis, Sotiris B},
  date = {2013},
  journaltitle = {Artificial Intelligence Review},
  volume = {39},
  pages = {261--283},
  publisher = {{Springer}},
  issn = {0269-2821}
}

@article{kreinovichHowEstimateForecasting2014,
  title = {How to Estimate Forecasting Quality: {{A}} System-Motivated Derivation of Symmetric Mean Absolute Percentage Error ({{SMAPE}}) and Other Similar Characteristics},
  author = {Kreinovich, Vladik and Nguyen, Hung T and Ouncharoen, Rujira},
  date = {2014}
}

@inproceedings{lecunTheoreticalFrameworkBackpropagation1988,
  title = {A Theoretical Framework for Back-Propagation},
  author = {LeCun, Yann and Touresky, D and Hinton, G and Sejnowski, T},
  date = {1988},
  volume = {1},
  pages = {21--28},
  eventtitle = {Proceedings of the 1988 Connectionist Models Summer School},
  file = {/Users/macbook/Zotero/storage/HWLJYU6G/A-Theoretical-Framework-for-Back-Propagation.pdf}
}

@article{likasGlobalKmeansClustering2003,
  title = {The Global K-Means Clustering Algorithm},
  author = {Likas, Aristidis and Vlassis, Nikos and Verbeek, Jakob J},
  date = {2003},
  journaltitle = {Pattern recognition},
  volume = {36},
  number = {2},
  pages = {451--461},
  publisher = {{Elsevier}},
  issn = {0031-3203}
}

@inproceedings{mehranMatchingbasedSchedulingAsynchronous2022,
  title = {Matching-Based {{Scheduling}} of {{Asynchronous Data Processing Workflows}} on the {{Computing Continuum}}},
  author = {Mehran, Narges and Samani, Zahra Najafabadi and Kimovski, Dragi and Prodan, Radu},
  date = {2022},
  pages = {58--70},
  publisher = {{IEEE}},
  eventtitle = {2022 {{IEEE International Conference}} on {{Cluster Computing}} ({{CLUSTER}})},
  isbn = {1-66549-856-0}
}

@inproceedings{orenSOLOSearchOnline2021,
  title = {{{SOLO}}: Search Online, Learn Offline for Combinatorial Optimization Problems},
  author = {Oren, Joel and Ross, Chana and Lefarov, Maksym and Richter, Felix and Taitler, Ayal and Feldman, Zohar and Di Castro, Dotan and Daniel, Christian},
  date = {2021},
  volume = {12},
  number = {1},
  pages = {97--105},
  eventtitle = {Proceedings of the {{International Symposium}} on {{Combinatorial Search}}}
}

@online{parikhDisadvantagesRNN2021,
  title = {Disadvantages of {{RNN}}},
  author = {Parikh, Dishant},
  date = {2021-01-16T14:46:18},
  url = {https://iq.opengenus.org/disadvantages-of-rnn/},
  urldate = {2023-02-10},
  abstract = {We have explored the disadvantages of RNN in depth. Recurrent Neural Networks (or RNNs) are the first of their kind neural networks that can help in analyzing and learning sequences of data rather than just instance-based learning.},
  langid = {english},
  organization = {{OpenGenus IQ: Computing Expertise \& Legacy}},
  note = {Available at \href{https://iq.opengenus.org/disadvantages-of-rnn/}{https://iq.opengenus.org/disadvantages-of-rnn/}},
  file = {/Users/macbook/Zotero/storage/GIXNQF9X/disadvantages-of-rnn.html}
}

@article{patelHybridCNNLSTMModel2022,
  title = {A Hybrid {{CNN-LSTM}} Model for Predicting Server Load in Cloud Computing},
  author = {Patel, Eva and Kushwaha, Dharmender Singh},
  date = {2022},
  journaltitle = {The Journal of Supercomputing},
  volume = {78},
  number = {8},
  pages = {1--30},
  publisher = {{Springer}},
  issn = {0920-8542}
}

@online{richDeepMindAIReduces2016,
  title = {{{DeepMind AI}} Reduces Energy Used for Cooling {{Google}} Data Centers by 40\%},
  author = {Rich, Evans and Gao, Jim},
  date = {2016-07-20},
  url = {https://blog.google/outreach-initiatives/environment/deepmind-ai-reduces-energy-used-for/},
  urldate = {2023-02-16},
  abstract = {In any large scale energy-consuming environment, this would be a huge improvement. Given how sophisticated Google’s data centers are already, it’s a phenomenal step forward.},
  langid = {american},
  organization = {{Google}},
  note = {Accessed at \href{https://blog.google/outreach-initiatives/environment/deepmind-ai-reduces-energy-used-for/}{https://blog.google/outreach-initiatives/environment/deepmind-ai-reduces-energy-used-for/}},
  file = {/Users/macbook/Zotero/storage/VVRMQM9I/deepmind-ai-reduces-energy-used-for.html}
}

@misc{sakLongShortTermMemory2014,
  title = {Long {{Short-Term Memory Based Recurrent Neural Network Architectures}} for {{Large Vocabulary Speech Recognition}}},
  author = {Sak, Haşim and Senior, Andrew and Beaufays, Françoise},
  date = {2014-02-05},
  number = {arXiv:1402.1128},
  eprint = {1402.1128},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1402.1128},
  url = {http://arxiv.org/abs/1402.1128},
  urldate = {2023-02-19},
  abstract = {Long Short-Term Memory (LSTM) is a recurrent neural network (RNN) architecture that has been designed to address the vanishing and exploding gradient problems of conventional RNNs. Unlike feedforward neural networks, RNNs have cyclic connections making them powerful for modeling sequences. They have been successfully used for sequence labeling and sequence prediction tasks, such as handwriting recognition, language modeling, phonetic labeling of acoustic frames. However, in contrast to the deep neural networks, the use of RNNs in speech recognition has been limited to phone recognition in small scale tasks. In this paper, we present novel LSTM based RNN architectures which make more effective use of model parameters to train acoustic models for large vocabulary speech recognition. We train and compare LSTM, RNN and DNN models at various numbers of parameters and configurations. We show that LSTM models converge quickly and give state of the art speech recognition performance for relatively small sized models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/macbook/Zotero/storage/PYWCB7Z8/Sak et al. - 2014 - Long Short-Term Memory Based Recurrent Neural Netw.pdf;/Users/macbook/Zotero/storage/RLB29XDA/1402.html}
}

@article{shapiEnergyConsumptionPrediction2021,
  title = {Energy Consumption Prediction by Using Machine Learning for Smart Building: {{Case}} Study in {{Malaysia}}},
  shorttitle = {Energy Consumption Prediction by Using Machine Learning for Smart Building},
  author = {Shapi, Mel Keytingan M. and Ramli, Nor Azuana and Awalin, Lilik J.},
  date = {2021-03-01},
  journaltitle = {Developments in the Built Environment},
  volume = {5},
  pages = {100037},
  issn = {2666-1659},
  doi = {10.1016/j.dibe.2020.100037},
  url = {https://www.sciencedirect.com/science/article/pii/S266616592030034X},
  urldate = {2023-02-16},
  abstract = {Building Energy Management System (BEMS) has been a substantial topic nowadays due to its importance in reducing energy wastage. However, the performance of one of BEMS applications which is energy consumption prediction has been stagnant due to problems such as low prediction accuracy. Thus, this research aims to address the problems by developing a predictive model for energy consumption in Microsoft Azure cloud-based machine learning platform. Three methodologies which are Support Vector Machine, Artificial Neural Network, and k-Nearest Neighbour are proposed for the algorithm of the predictive model. Focusing on real-life application in Malaysia, two tenants from a commercial building are taken as a case study. The data collected is analysed and pre-processed before it is used for model training and testing. The performance of each of the methods is compared based on RMSE, NRMSE, and MAPE metrics. The experimentation shows that each tenant’s energy consumption has different distribution characteristics.},
  langid = {english},
  keywords = {Building energy management system,Energy consumption,Machine learning,Microsoft Azure,Prediction},
  file = {/Users/macbook/Zotero/storage/DPQY7E69/Shapi et al. - 2021 - Energy consumption prediction by using machine lea.pdf;/Users/macbook/Zotero/storage/Q8NP4AQT/S266616592030034X.html}
}

@article{stoopComplexitySchedulingPractice1996,
  title = {The Complexity of Scheduling in Practice},
  author = {Stoop, Paul P.M. and Wiers, Vincent C.S.},
  date = {1996-01-01},
  journaltitle = {International Journal of Operations \& Production Management},
  volume = {16},
  number = {10},
  pages = {37--53},
  publisher = {{MCB UP Ltd}},
  issn = {0144-3577},
  doi = {10.1108/01443579610130682},
  url = {https://doi.org/10.1108/01443579610130682},
  urldate = {2023-02-20},
  abstract = {Successful implementations of scheduling techniques in practice are scarce. Not only do daily disturbances lead to a gap between theory and practice, but also the extent to which a scheduling technique can adequately model the processes on the shopfloor, and the extent to which the optimization goal of a technique matches the organizational goal are not great enough. Further, the schedulers’ actions may play an important role in the fulfilment of the generated schedules. The organizational structure with its different responsibilities and conflicting goals may also result in the poor performance of scheduling techniques. Besides these, there is the problem of measuring the quality of a schedule. Discusses the causes for these gaps and illustrates the solutions to improve scheduling by way of a case study.},
  keywords = {Decision making,Performance measurement,Production control,Production scheduling,Shopfloor},
  file = {/Users/macbook/Zotero/storage/Y49R6P67/Stoop and Wiers - 1996 - The complexity of scheduling in practice.pdf}
}

@inproceedings{thonglekImprovingResourceUtilization2019,
  title = {Improving {{Resource Utilization}} in {{Data Centers}} Using an {{LSTM-based Prediction Model}}},
  booktitle = {2019 {{IEEE International Conference}} on {{Cluster Computing}} ({{CLUSTER}})},
  author = {Thonglek, Kundjanasith and Ichikawa, Kohei and Takahashi, Keichi and Iida, Hajimu and Nakasan, Chawanat},
  date = {2019-09},
  pages = {1--8},
  issn = {2168-9253},
  doi = {10.1109/CLUSTER.2019.8891022},
  abstract = {Data centers are centralized facilities where computing and networking hardware are aggregated to handle large amounts of data and computation. In a data center, computing resources such as CPU and memory are usually managed by a resource manager. The resource manager accepts resource requests from users and allocates resources to their applications. A commonly known problem in resource management is that users often request more resources than their applications actually use. This leads to the degradation of overall resource utilization in a data center. This paper aims to improve resource utilization in data centers by predicting the required resource for each application. We designed and implemented a neural network model based on Long Short-Term Memory (LSTM) to predict more efficient resource allocation for a job based on historical data. Our model has two LSTM layers each of which learns the relationship between: (1) allocation and usage, and (2) CPU and memory. We used Googles cluster-usage trace, which contains a trace of resource allocation and usage for each job executed on a Google data center, to train our neural network. Googles cluster scheduler simulator was used to evaluate our proposed method. Our simulation indicated that the proposed method improved the CPU utilization and memory utilization by 10.71\% and 47.36\%, respectively, compared to a conventional resource manager. Moreover, we discovered that increasing the memory cell size of our LSTM model improves the accuracy of the prediction in return for longer training time.},
  eventtitle = {2019 {{IEEE International Conference}} on {{Cluster Computing}} ({{CLUSTER}})},
  keywords = {Computer architecture,Computing Resources,Data centers,Google,Long Short-Term Memory,Microprocessors,Neural networks,Predictive models,Resource management,Resource Management,Resource Utilization},
  file = {/Users/macbook/Zotero/storage/TRXGC6L4/8891022.html}
}

@book{weisbergAppliedLinearRegression2005,
  title = {Applied Linear Regression},
  author = {Weisberg, Sanford},
  date = {2005},
  volume = {528},
  publisher = {{John Wiley \& Sons}},
  isbn = {0-471-70408-3}
}

@misc{zhangArchitecturalComplexityMeasures2016,
  title = {Architectural {{Complexity Measures}} of {{Recurrent Neural Networks}}},
  author = {Zhang, Saizheng and Wu, Yuhuai and Che, Tong and Lin, Zhouhan and Memisevic, Roland and Salakhutdinov, Ruslan and Bengio, Yoshua},
  date = {2016-11-12},
  number = {arXiv:1602.08210},
  eprint = {1602.08210},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1602.08210},
  url = {http://arxiv.org/abs/1602.08210},
  urldate = {2022-11-10},
  abstract = {In this paper, we systematically analyze the connecting architectures of recurrent neural networks (RNNs). Our main contribution is twofold: first, we present a rigorous graph-theoretic framework describing the connecting architectures of RNNs in general. Second, we propose three architecture complexity measures of RNNs: (a) the recurrent depth, which captures the RNN's over-time nonlinear complexity, (b) the feedforward depth, which captures the local input-output nonlinearity (similar to the "depth" in feedforward neural networks (FNNs)), and (c) the recurrent skip coefficient which captures how rapidly the information propagates over time. We rigorously prove each measure's existence and computability. Our experimental results show that RNNs might benefit from larger recurrent depth and feedforward depth. We further demonstrate that increasing recurrent skip coefficient offers performance boosts on long term dependency problems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  note = {Comment: 17 pages, 8 figures; To appear in NIPS2016},
  file = {/Users/macbook/Zotero/storage/IDG46J4F/Zhang et al. - 2016 - Architectural Complexity Measures of Recurrent Neu.pdf;/Users/macbook/Zotero/storage/63GS7YN6/1602.html}
}
