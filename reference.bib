@book{andersonIntroductionNeuralNetworks1995,
  title = {An Introduction to Neural Networks},
  author = {Anderson, James A},
  date = {1995},
  publisher = {{MIT press}},
  isbn = {0-262-51081-2}
}

@unpublished{botchkarevPerformanceMetricsError2018,
  title = {Performance Metrics (Error Measures) in Machine Learning Regression, Forecasting and Prognostics: {{Properties}} and Typology},
  author = {Botchkarev, Alexei},
  date = {2018},
  shortjournal = {arXiv preprint arXiv:1809.03006},
  eprint = {1809.03006},
  eprinttype = {arxiv},
  archiveprefix = {arXiv}
}

@article{breimanRandomForests2001,
  title = {Random Forests},
  author = {Breiman, Leo},
  date = {2001},
  journaltitle = {Machine learning},
  volume = {45},
  pages = {5--32},
  publisher = {{Springer}},
  issn = {0885-6125}
}

@article{chaiRootMeanSquare2014,
  title = {Root Mean Square Error ({{RMSE}}) or Mean Absolute Error ({{MAE}})?–{{Arguments}} against Avoiding {{RMSE}} in the Literature},
  author = {Chai, Tianfeng and Draxler, Roland R},
  date = {2014},
  journaltitle = {Geoscientific model development},
  volume = {7},
  number = {3},
  pages = {1247--1250},
  publisher = {{Copernicus Publications Göttingen, Germany}},
  issn = {1991-9603}
}

@article{demyttenaereMeanAbsolutePercentage2016,
  title = {Mean Absolute Percentage Error for Regression Models},
  author = {De Myttenaere, Arnaud and Golden, Boris and Le Grand, Bénédicte and Rossi, Fabrice},
  date = {2016},
  journaltitle = {Neurocomputing},
  volume = {192},
  pages = {38--48},
  publisher = {{Elsevier}},
  issn = {0925-2312}
}

@online{EdgeComputingVs,
  title = {Edge {{Computing}} vs. {{Fog Computing}}: 10 {{Key Comparisons}} |},
  shorttitle = {Edge {{Computing}} vs. {{Fog Computing}}},
  url = {https://www.spiceworks.com/tech/cloud/articles/edge-vs-fog-computing/},
  urldate = {2023-01-23},
  abstract = {While edge computing brings the computers closer to the source of data, cloud computing makes advanced technology available over the internet for a fixed, recurring fee.},
  langid = {american},
  file = {/Users/macbook/Zotero/storage/JI327NNE/edge-vs-fog-computing.html}
}

@article{goldbergPrimerNeuralNetwork2016,
  title = {A Primer on Neural Network Models for Natural Language Processing},
  author = {Goldberg, Yoav},
  date = {2016},
  journaltitle = {Journal of Artificial Intelligence Research},
  volume = {57},
  pages = {345--420},
  issn = {1076-9757}
}

@article{gravesLongShorttermMemory2012,
  title = {Long Short-Term Memory},
  author = {Graves, Alex and Graves, Alex},
  date = {2012},
  journaltitle = {Supervised sequence labelling with recurrent neural networks},
  pages = {37--45},
  publisher = {{Springer}},
  issn = {3642247962}
}

@inproceedings{hermansTrainingAnalysingDeep2013,
  title = {Training and {{Analysing Deep Recurrent Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Hermans, Michiel and Schrauwen, Benjamin},
  date = {2013},
  volume = {26},
  publisher = {{Curran Associates, Inc.}},
  url = {https://papers.nips.cc/paper/2013/hash/1ff8a7b5dc7a7d1f0ed65aaa29c04b1e-Abstract.html},
  urldate = {2023-02-13},
  abstract = {Time series often have a temporal hierarchy, with information that is spread out over multiple time scales. Common recurrent neural networks, however, do not explicitly accommodate such a hierarchy, and most research on them has been focusing on training algorithms rather than on their basic architecture. In this pa- per we study the effect of a hierarchy of recurrent neural networks on processing time series. Here, each layer is a recurrent network which receives the hidden state of the previous layer as input. This architecture allows us to perform hi- erarchical processing on difficult temporal tasks, and more naturally capture the structure of time series. We show that they reach state-of-the-art performance for recurrent networks in character-level language modelling when trained with sim- ple stochastic gradient descent. We also offer an analysis of the different emergent time scales.},
  file = {/Users/macbook/Zotero/storage/GN9EHVBU/Hermans and Schrauwen - 2013 - Training and Analysing Deep Recurrent Neural Netwo.pdf}
}

@article{hochreiterLongShortTermMemory1997,
  title = {Long {{Short-Term Memory}}},
  author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
  date = {1997-11},
  journaltitle = {Neural Computation},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  issn = {0899-7667},
  doi = {10.1162/neco.1997.9.8.1735},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  eventtitle = {Neural {{Computation}}},
  file = {/Users/macbook/Zotero/storage/VGN6MQQ9/6795963.html}
}

@article{husseinEnhancementPerformanceRandom2021,
  title = {Enhancement Performance of Random Forest Algorithm via One Hot Encoding for {{IoT IDS}}},
  author = {Hussein, Adil Yousef and Falcarin, Paolo and Sadiq, Ahmed T},
  date = {2021},
  journaltitle = {Periodicals of Engineering and Natural Sciences},
  volume = {9},
  number = {3},
  pages = {579--591},
  issn = {2303-4521}
}

@article{jamesMonteCarloTheory1980,
  title = {Monte {{Carlo}} Theory and Practice},
  author = {James, Frederick},
  date = {1980},
  journaltitle = {Reports on progress in Physics},
  volume = {43},
  number = {9},
  pages = {1145},
  publisher = {{IOP Publishing}},
  issn = {0034-4885}
}

@article{jiangEdgeEnhancedGANRemote2019,
  title = {Edge-{{Enhanced GAN}} for {{Remote Sensing Image Superresolution}}},
  author = {Jiang, Kui and Wang, Zhongyuan and Yi, Peng and Wang, Guangcheng and Lu, Tao and Jiang, Junjun},
  date = {2019-08},
  journaltitle = {IEEE Transactions on Geoscience and Remote Sensing},
  volume = {57},
  number = {8},
  pages = {5799--5812},
  issn = {1558-0644},
  doi = {10.1109/TGRS.2019.2902431},
  abstract = {The current superresolution (SR) methods based on deep learning have shown remarkable comparative advantages but remain unsatisfactory in recovering the high-frequency edge details of the images in noise-contaminated imaging conditions, e.g., remote sensing satellite imaging. In this paper, we propose a generative adversarial network (GAN)-based edge-enhancement network (EEGAN) for robust satellite image SR reconstruction along with the adversarial learning strategy that is insensitive to noise. In particular, EEGAN consists of two main subnetworks: an ultradense subnetwork (UDSN) and an edge-enhancement subnetwork (EESN). In UDSN, a group of 2-D dense blocks is assembled for feature extraction and to obtain an intermediate high-resolution result that looks sharp but is eroded with artifacts and noises as previous GAN-based methods do. Then, EESN is constructed to extract and enhance the image contours by purifying the noise-contaminated components with mask processing. The recovered intermediate image and enhanced edges can be combined to generate the result that enjoys high credibility and clear contents. Extensive experiments on Kaggle Open Source Data set, Jilin-1 video satellite images, and Digitalglobe show superior reconstruction performance compared to the state-of-the-art SR approaches.},
  eventtitle = {{{IEEE Transactions}} on {{Geoscience}} and {{Remote Sensing}}},
  keywords = {Adversarial learning,dense connection,edge enhancement,Feature extraction,Gallium nitride,Generative adversarial networks,Image edge detection,Image reconstruction,Image resolution,remote sensing imagery,Satellites,superresolution},
  file = {/Users/macbook/Zotero/storage/WF4T9XWY/Jiang et al. - 2019 - Edge-Enhanced GAN for Remote Sensing Image Superre.pdf;/Users/macbook/Zotero/storage/4RLRMQBL/8677274.html}
}

@article{koksoyMultiresponseRobustDesign2006,
  title = {Multiresponse Robust Design: {{Mean}} Square Error ({{MSE}}) Criterion},
  author = {Köksoy, Onur},
  date = {2006},
  journaltitle = {Applied Mathematics and Computation},
  volume = {175},
  number = {2},
  pages = {1716--1729},
  publisher = {{Elsevier}},
  issn = {0096-3003}
}

@article{kotsiantisDecisionTreesRecent2013,
  title = {Decision Trees: A Recent Overview},
  author = {Kotsiantis, Sotiris B},
  date = {2013},
  journaltitle = {Artificial Intelligence Review},
  volume = {39},
  pages = {261--283},
  publisher = {{Springer}},
  issn = {0269-2821}
}

@article{kreinovichHowEstimateForecasting2014,
  title = {How to Estimate Forecasting Quality: {{A}} System-Motivated Derivation of Symmetric Mean Absolute Percentage Error ({{SMAPE}}) and Other Similar Characteristics},
  author = {Kreinovich, Vladik and Nguyen, Hung T and Ouncharoen, Rujira},
  date = {2014}
}

@inproceedings{lecunTheoreticalFrameworkBackpropagation1988,
  title = {A Theoretical Framework for Back-Propagation},
  author = {LeCun, Yann and Touresky, D and Hinton, G and Sejnowski, T},
  date = {1988},
  volume = {1},
  pages = {21--28},
  eventtitle = {Proceedings of the 1988 Connectionist Models Summer School},
  file = {/Users/macbook/Zotero/storage/HWLJYU6G/A-Theoretical-Framework-for-Back-Propagation.pdf}
}

@inproceedings{orenSOLOSearchOnline2021,
  title = {{{SOLO}}: Search Online, Learn Offline for Combinatorial Optimization Problems},
  author = {Oren, Joel and Ross, Chana and Lefarov, Maksym and Richter, Felix and Taitler, Ayal and Feldman, Zohar and Di Castro, Dotan and Daniel, Christian},
  date = {2021},
  volume = {12},
  number = {1},
  pages = {97--105},
  eventtitle = {Proceedings of the {{International Symposium}} on {{Combinatorial Search}}}
}

@online{parikhDisadvantagesRNN2021,
  title = {Disadvantages of {{RNN}}},
  author = {Parikh, Dishant},
  date = {2021-01-16T14:46:18},
  url = {https://iq.opengenus.org/disadvantages-of-rnn/},
  urldate = {2023-02-10},
  abstract = {We have explored the disadvantages of RNN in depth. Recurrent Neural Networks (or RNNs) are the first of their kind neural networks that can help in analyzing and learning sequences of data rather than just instance-based learning.},
  langid = {english},
  organization = {{OpenGenus IQ: Computing Expertise \& Legacy}},
  note = {Available at \href{https://iq.opengenus.org/disadvantages-of-rnn/}{https://iq.opengenus.org/disadvantages-of-rnn/}},
  file = {/Users/macbook/Zotero/storage/GIXNQF9X/disadvantages-of-rnn.html}
}

@article{patelHybridCNNLSTMModel2022,
  title = {A Hybrid {{CNN-LSTM}} Model for Predicting Server Load in Cloud Computing},
  author = {Patel, Eva and Kushwaha, Dharmender Singh},
  date = {2022},
  journaltitle = {The Journal of Supercomputing},
  volume = {78},
  number = {8},
  pages = {1--30},
  publisher = {{Springer}},
  issn = {0920-8542}
}

@online{richDeepMindAIReduces2016,
  title = {{{DeepMind AI}} Reduces Energy Used for Cooling {{Google}} Data Centers by 40\%},
  author = {Rich, Evans and Gao, Jim},
  date = {2016-07-20},
  url = {https://blog.google/outreach-initiatives/environment/deepmind-ai-reduces-energy-used-for/},
  urldate = {2023-02-16},
  abstract = {In any large scale energy-consuming environment, this would be a huge improvement. Given how sophisticated Google’s data centers are already, it’s a phenomenal step forward.},
  langid = {american},
  organization = {{Google}},
  note = {Accessed at \href{https://blog.google/outreach-initiatives/environment/deepmind-ai-reduces-energy-used-for/}{https://blog.google/outreach-initiatives/environment/deepmind-ai-reduces-energy-used-for/}},
  file = {/Users/macbook/Zotero/storage/VVRMQM9I/deepmind-ai-reduces-energy-used-for.html}
}

@article{shapiEnergyConsumptionPrediction2021,
  title = {Energy Consumption Prediction by Using Machine Learning for Smart Building: {{Case}} Study in {{Malaysia}}},
  shorttitle = {Energy Consumption Prediction by Using Machine Learning for Smart Building},
  author = {Shapi, Mel Keytingan M. and Ramli, Nor Azuana and Awalin, Lilik J.},
  date = {2021-03-01},
  journaltitle = {Developments in the Built Environment},
  volume = {5},
  pages = {100037},
  issn = {2666-1659},
  doi = {10.1016/j.dibe.2020.100037},
  url = {https://www.sciencedirect.com/science/article/pii/S266616592030034X},
  urldate = {2023-02-16},
  abstract = {Building Energy Management System (BEMS) has been a substantial topic nowadays due to its importance in reducing energy wastage. However, the performance of one of BEMS applications which is energy consumption prediction has been stagnant due to problems such as low prediction accuracy. Thus, this research aims to address the problems by developing a predictive model for energy consumption in Microsoft Azure cloud-based machine learning platform. Three methodologies which are Support Vector Machine, Artificial Neural Network, and k-Nearest Neighbour are proposed for the algorithm of the predictive model. Focusing on real-life application in Malaysia, two tenants from a commercial building are taken as a case study. The data collected is analysed and pre-processed before it is used for model training and testing. The performance of each of the methods is compared based on RMSE, NRMSE, and MAPE metrics. The experimentation shows that each tenant’s energy consumption has different distribution characteristics.},
  langid = {english},
  keywords = {Building energy management system,Energy consumption,Machine learning,Microsoft Azure,Prediction},
  file = {/Users/macbook/Zotero/storage/DPQY7E69/Shapi et al. - 2021 - Energy consumption prediction by using machine lea.pdf;/Users/macbook/Zotero/storage/Q8NP4AQT/S266616592030034X.html}
}

@inproceedings{thonglekImprovingResourceUtilization2019,
  title = {Improving {{Resource Utilization}} in {{Data Centers}} Using an {{LSTM-based Prediction Model}}},
  booktitle = {2019 {{IEEE International Conference}} on {{Cluster Computing}} ({{CLUSTER}})},
  author = {Thonglek, Kundjanasith and Ichikawa, Kohei and Takahashi, Keichi and Iida, Hajimu and Nakasan, Chawanat},
  date = {2019-09},
  pages = {1--8},
  issn = {2168-9253},
  doi = {10.1109/CLUSTER.2019.8891022},
  abstract = {Data centers are centralized facilities where computing and networking hardware are aggregated to handle large amounts of data and computation. In a data center, computing resources such as CPU and memory are usually managed by a resource manager. The resource manager accepts resource requests from users and allocates resources to their applications. A commonly known problem in resource management is that users often request more resources than their applications actually use. This leads to the degradation of overall resource utilization in a data center. This paper aims to improve resource utilization in data centers by predicting the required resource for each application. We designed and implemented a neural network model based on Long Short-Term Memory (LSTM) to predict more efficient resource allocation for a job based on historical data. Our model has two LSTM layers each of which learns the relationship between: (1) allocation and usage, and (2) CPU and memory. We used Googles cluster-usage trace, which contains a trace of resource allocation and usage for each job executed on a Google data center, to train our neural network. Googles cluster scheduler simulator was used to evaluate our proposed method. Our simulation indicated that the proposed method improved the CPU utilization and memory utilization by 10.71\% and 47.36\%, respectively, compared to a conventional resource manager. Moreover, we discovered that increasing the memory cell size of our LSTM model improves the accuracy of the prediction in return for longer training time.},
  eventtitle = {2019 {{IEEE International Conference}} on {{Cluster Computing}} ({{CLUSTER}})},
  keywords = {Computer architecture,Computing Resources,Data centers,Google,Long Short-Term Memory,Microprocessors,Neural networks,Predictive models,Resource management,Resource Management,Resource Utilization},
  file = {/Users/macbook/Zotero/storage/TRXGC6L4/8891022.html}
}

@book{weisbergAppliedLinearRegression2005,
  title = {Applied Linear Regression},
  author = {Weisberg, Sanford},
  date = {2005},
  volume = {528},
  publisher = {{John Wiley \& Sons}},
  isbn = {0-471-70408-3}
}

@misc{zhangArchitecturalComplexityMeasures2016,
  title = {Architectural {{Complexity Measures}} of {{Recurrent Neural Networks}}},
  author = {Zhang, Saizheng and Wu, Yuhuai and Che, Tong and Lin, Zhouhan and Memisevic, Roland and Salakhutdinov, Ruslan and Bengio, Yoshua},
  date = {2016-11-12},
  number = {arXiv:1602.08210},
  eprint = {1602.08210},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1602.08210},
  url = {http://arxiv.org/abs/1602.08210},
  urldate = {2022-11-10},
  abstract = {In this paper, we systematically analyze the connecting architectures of recurrent neural networks (RNNs). Our main contribution is twofold: first, we present a rigorous graph-theoretic framework describing the connecting architectures of RNNs in general. Second, we propose three architecture complexity measures of RNNs: (a) the recurrent depth, which captures the RNN's over-time nonlinear complexity, (b) the feedforward depth, which captures the local input-output nonlinearity (similar to the "depth" in feedforward neural networks (FNNs)), and (c) the recurrent skip coefficient which captures how rapidly the information propagates over time. We rigorously prove each measure's existence and computability. Our experimental results show that RNNs might benefit from larger recurrent depth and feedforward depth. We further demonstrate that increasing recurrent skip coefficient offers performance boosts on long term dependency problems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  note = {Comment: 17 pages, 8 figures; To appear in NIPS2016},
  file = {/Users/macbook/Zotero/storage/IDG46J4F/Zhang et al. - 2016 - Architectural Complexity Measures of Recurrent Neu.pdf;/Users/macbook/Zotero/storage/63GS7YN6/1602.html}
}
